{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f803886b-d0ba-4dfa-9736-5e9c7384628e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#MAIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6992dde8-fdcc-4cea-a859-1f8540ccc271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PATH VERIFICATION ===\nRaw Data Path: /Volumes/main/retail_lakehouse/raw_data\nBronze Layer Path: /Volumes/main/retail_lakehouse/bronze\nCheckpoints Path: /Volumes/main/retail_lakehouse/checkpoints\n✅ Found 1 sales files\n  - dbfs:/Volumes/main/retail_lakehouse/raw_data/sales/sales_data_20250812_080459.csv\n✅ Found 1 customer files\n  - dbfs:/Volumes/main/retail_lakehouse/raw_data/customers/customer_data_20250812_080459.csv\n✅ Found 1 product files\n  - dbfs:/Volumes/main/retail_lakehouse/raw_data/products/product_data_20250812_080459.csv\n✅ Created bronze and checkpoints directories\n✅ Using database: retail_lakehouse\n\n=== Processing Sales Data ===\n✅ Sales Data stream created successfully\n⏳ Sales Data stream started, waiting for completion...\n✅ Sales Data completed successfully - 10000 records written\n\n=== Processing Customer Data ===\n✅ Customer Data stream created successfully\n⏳ Customer Data stream started, waiting for completion...\n✅ Customer Data completed successfully - 5000 records written\n\n=== Processing Product Data ===\n✅ Product Data stream created successfully\n⏳ Product Data stream started, waiting for completion...\n✅ Product Data completed successfully - 246 records written\n\n=== Creating Delta Tables ===\n❌ Error creating table sales_bronze: [RequestId=1aacdf43-2288-4a1a-a01b-d654adcb91c6 ErrorClass=INVALID_PARAMETER_VALUE.INVALID_PARAMETER_VALUE] Missing cloud file system scheme\n\nJVM stacktrace:\ncom.databricks.sql.managedcatalog.UnityCatalogServiceException\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:126)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:266)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7313)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7299)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:6280)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:3242)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$generateTemporaryPathCredentials$2(ProfiledManagedCatalog.scala:744)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1698)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.generateTemporaryPathCredentials(ProfiledManagedCatalog.scala:744)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:146)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:194)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:1190)\n\tat com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:516)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateDSv2Table(ResolveWithCredential.scala:134)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:166)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:69)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:936)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3382)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3211)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:639)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:682)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:624)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\n❌ Error creating table customer_bronze: [RequestId=2e222e71-179f-49f6-bcd0-f4391cff7218 ErrorClass=INVALID_PARAMETER_VALUE.INVALID_PARAMETER_VALUE] Missing cloud file system scheme\n\nJVM stacktrace:\ncom.databricks.sql.managedcatalog.UnityCatalogServiceException\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:126)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:266)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7313)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7299)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:6280)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:3242)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$generateTemporaryPathCredentials$2(ProfiledManagedCatalog.scala:744)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1698)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.generateTemporaryPathCredentials(ProfiledManagedCatalog.scala:744)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:146)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:194)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:1190)\n\tat com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:516)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateDSv2Table(ResolveWithCredential.scala:134)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:166)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:69)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$execu\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:936)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3382)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3211)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:639)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:682)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:624)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\n\n=== DATA INGESTION VERIFICATION ===\n\uD83D\uDCCA Sales Bronze Records: 10,000\n\uD83D\uDD0D Sample Sales Data:\n+---------------+----------------+----------------+--------+-----------------+------+----------+---------------+--------+--------+----------+------------+----------------+---------------+------------+--------------+-----------+----------------+---------------+--------------+----------------------------------------------------------------------------+-----------------------+------------+\n|transaction_id |transaction_date|transaction_time|store_id|store_name       |region|product_id|product_name   |category|quantity|unit_price|total_amount|discount_percent|discount_amount|final_amount|payment_method|customer_id|customer_segment|sales_person_id|promotion_code|file_name                                                                   |processing_time        |bronze_layer|\n+---------------+----------------+----------------+--------+-----------------+------+----------+---------------+--------+--------+----------+------------+----------------+---------------+------------+--------------+-----------+----------------+---------------+--------------+----------------------------------------------------------------------------+-----------------------+------------+\n|TXN6EB7D273FDDF|2024-03-03      |08:04:54        |ST015   |West Kelly Mall  |South |PRD3615   |Foundation     |Beauty  |5       |146.31    |731.55      |0               |0.0            |731.55      |Digital Wallet|CUST54597  |Budget          |EMP259         |NULL          |/Volumes/main/retail_lakehouse/raw_data/sales/sales_data_20250812_080459.csv|2025-08-12 08:30:06.967|sales_bronze|\n|TXNAFB609971911|2024-07-22      |08:04:54        |ST013   |Evanton Mall     |North |PRD5333   |T-Shirt        |Clothing|1       |185.9     |185.9       |10              |18.59          |167.31      |Credit Card   |CUST70217  |Premium         |EMP487         |NULL          |/Volumes/main/retail_lakehouse/raw_data/sales/sales_data_20250812_080459.csv|2025-08-12 08:30:06.967|sales_bronze|\n|TXN6F59BE437AC1|2024-06-08      |08:04:54        |ST007   |Collinsmouth Mall|North |PRD5741   |Building Blocks|Toys    |1       |32.45     |32.45       |5               |1.62           |30.83       |Credit Card   |CUST40512  |Premium         |EMP489         |NULL          |/Volumes/main/retail_lakehouse/raw_data/sales/sales_data_20250812_080459.csv|2025-08-12 08:30:06.967|sales_bronze|\n+---------------+----------------+----------------+--------+-----------------+------+----------+---------------+--------+--------+----------+------------+----------------+---------------+------------+--------------+-----------+----------------+---------------+--------------+----------------------------------------------------------------------------+-----------------------+------------+\n\n\uD83D\uDC65 Customer Bronze Records: 5,000\n\uD83D\uDD0D Sample Customer Data:\n+-----------+----------+---------+---------------------+---------------------+-------------+------+---------------------------------+---------------+--------------+--------+-------+----------------+-----------------+---------+-----------------------------------------------------------------------------------+-----------------------+---------------+\n|customer_id|first_name|last_name|email                |phone                |date_of_birth|gender|address_line1                    |city           |state         |zip_code|country|customer_segment|registration_date|is_active|file_name                                                                          |processing_time        |bronze_layer   |\n+-----------+----------+---------+---------------------+---------------------+-------------+------+---------------------------------+---------------+--------------+--------+-------+----------------+-----------------+---------+-----------------------------------------------------------------------------------+-----------------------+---------------+\n|CUST10000  |Janice    |Moses    |aclarke@example.net  |+1-304-724-9984x62745|1999-02-24   |M     |7404 Dale Forest                 |North Jennifer |Wyoming       |86935   |USA    |VIP             |2024-05-03       |true     |/Volumes/main/retail_lakehouse/raw_data/customers/customer_data_20250812_080459.csv|2025-08-12 08:22:48.284|customer_bronze|\n|CUST10001  |Kevin     |Hall     |cgonzalez@example.net|449-819-1635         |1976-05-28   |Other |0179 Perez Manor                 |North Alexander|New York      |53396   |USA    |Standard        |2024-07-27       |false    |/Volumes/main/retail_lakehouse/raw_data/customers/customer_data_20250812_080459.csv|2025-08-12 08:22:48.284|customer_bronze|\n|CUST10002  |Scott     |Brown    |alisha34@example.org |449-212-3230         |2006-04-10   |F     |04983 Stephanie Islands Suite 324|Schneidershire |North Carolina|96156   |USA    |VIP             |2025-03-29       |true     |/Volumes/main/retail_lakehouse/raw_data/customers/customer_data_20250812_080459.csv|2025-08-12 08:22:48.284|customer_bronze|\n+-----------+----------+---------+---------------------+---------------------+-------------+------+---------------------------------+---------------+--------------+--------+-------+----------------+-----------------+---------+-----------------------------------------------------------------------------------+-----------------------+---------------+\n\n\uD83D\uDECD️  Product Bronze Records: 246\n\uD83D\uDD0D Sample Product Data:\n+----------+------------+-----------+-----------------+-----------------------------+-----------+----------+-------------+------+----------+-----+----+-----------+---------+---------------------------------------------------------------------------------+-----------------------+--------------+\n|product_id|product_name|category   |sub_category     |brand                        |supplier_id|cost_price|selling_price|weight|dimensions|color|size|launch_date|is_active|file_name                                                                        |processing_time        |bronze_layer  |\n+----------+------------+-----------+-----------------+-----------------------------+-----------+----------+-------------+------+----------+-----+----+-----------+---------+---------------------------------------------------------------------------------+-----------------------+--------------+\n|PRD9069   |Smartphone  |Electronics|Electronics Sub 5|Garcia PLC                   |SUP435     |97.9      |253.48       |5.72  |50x57x73  |Black|XL  |2025-01-27 |true     |/Volumes/main/retail_lakehouse/raw_data/products/product_data_20250812_080459.csv|2025-08-12 08:30:15.245|product_bronze|\n|PRD8343   |Smartphone  |Electronics|Electronics Sub 4|Frey-Wade                    |SUP687     |184.12    |120.26       |14.12 |20x46x74  |Gray |S   |2025-06-20 |true     |/Volumes/main/retail_lakehouse/raw_data/products/product_data_20250812_080459.csv|2025-08-12 08:30:15.245|product_bronze|\n|PRD5844   |Smartphone  |Electronics|Electronics Sub 2|Morales, Hamilton and Russell|SUP822     |36.01     |85.83        |43.34 |46x8x47   |White|XL  |2025-03-18 |true     |/Volumes/main/retail_lakehouse/raw_data/products/product_data_20250812_080459.csv|2025-08-12 08:30:15.245|product_bronze|\n+----------+------------+-----------+-----------------+-----------------------------+-----------+----------+-------------+------+----------+-----+----+-----------+---------+---------------------------------------------------------------------------------+-----------------------+--------------+\n\n\n=== ALTERNATIVE VERIFICATION USING DIRECT DELTA PATHS ===\n\n\uD83D\uDD0D Sales Data Sample:\n+---------------+----------------+----------------+--------+-----------------+------+----------+---------------+--------+--------+----------+------------+----------------+---------------+------------+--------------+-----------+----------------+---------------+--------------+----------------------------------------------------------------------------+-----------------------+------------+\n|transaction_id |transaction_date|transaction_time|store_id|store_name       |region|product_id|product_name   |category|quantity|unit_price|total_amount|discount_percent|discount_amount|final_amount|payment_method|customer_id|customer_segment|sales_person_id|promotion_code|file_name                                                                   |processing_time        |bronze_layer|\n+---------------+----------------+----------------+--------+-----------------+------+----------+---------------+--------+--------+----------+------------+----------------+---------------+------------+--------------+-----------+----------------+---------------+--------------+----------------------------------------------------------------------------+-----------------------+------------+\n|TXN6EB7D273FDDF|2024-03-03      |08:04:54        |ST015   |West Kelly Mall  |South |PRD3615   |Foundation     |Beauty  |5       |146.31    |731.55      |0               |0.0            |731.55      |Digital Wallet|CUST54597  |Budget          |EMP259         |NULL          |/Volumes/main/retail_lakehouse/raw_data/sales/sales_data_20250812_080459.csv|2025-08-12 08:30:06.967|sales_bronze|\n|TXNAFB609971911|2024-07-22      |08:04:54        |ST013   |Evanton Mall     |North |PRD5333   |T-Shirt        |Clothing|1       |185.9     |185.9       |10              |18.59          |167.31      |Credit Card   |CUST70217  |Premium         |EMP487         |NULL          |/Volumes/main/retail_lakehouse/raw_data/sales/sales_data_20250812_080459.csv|2025-08-12 08:30:06.967|sales_bronze|\n|TXN6F59BE437AC1|2024-06-08      |08:04:54        |ST007   |Collinsmouth Mall|North |PRD5741   |Building Blocks|Toys    |1       |32.45     |32.45       |5               |1.62           |30.83       |Credit Card   |CUST40512  |Premium         |EMP489         |NULL          |/Volumes/main/retail_lakehouse/raw_data/sales/sales_data_20250812_080459.csv|2025-08-12 08:30:06.967|sales_bronze|\n+---------------+----------------+----------------+--------+-----------------+------+----------+---------------+--------+--------+----------+------------+----------------+---------------+------------+--------------+-----------+----------------+---------------+--------------+----------------------------------------------------------------------------+-----------------------+------------+\n\n\uD83D\uDCCA Sales Total Records: 10,000\n\n\uD83D\uDD0D Customer Data Sample:\n+-----------+----------+---------+---------------------+---------------------+-------------+------+---------------------------------+---------------+--------------+--------+-------+----------------+-----------------+---------+-----------------------------------------------------------------------------------+-----------------------+---------------+\n|customer_id|first_name|last_name|email                |phone                |date_of_birth|gender|address_line1                    |city           |state         |zip_code|country|customer_segment|registration_date|is_active|file_name                                                                          |processing_time        |bronze_layer   |\n+-----------+----------+---------+---------------------+---------------------+-------------+------+---------------------------------+---------------+--------------+--------+-------+----------------+-----------------+---------+-----------------------------------------------------------------------------------+-----------------------+---------------+\n|CUST10000  |Janice    |Moses    |aclarke@example.net  |+1-304-724-9984x62745|1999-02-24   |M     |7404 Dale Forest                 |North Jennifer |Wyoming       |86935   |USA    |VIP             |2024-05-03       |true     |/Volumes/main/retail_lakehouse/raw_data/customers/customer_data_20250812_080459.csv|2025-08-12 08:22:48.284|customer_bronze|\n|CUST10001  |Kevin     |Hall     |cgonzalez@example.net|449-819-1635         |1976-05-28   |Other |0179 Perez Manor                 |North Alexander|New York      |53396   |USA    |Standard        |2024-07-27       |false    |/Volumes/main/retail_lakehouse/raw_data/customers/customer_data_20250812_080459.csv|2025-08-12 08:22:48.284|customer_bronze|\n|CUST10002  |Scott     |Brown    |alisha34@example.org |449-212-3230         |2006-04-10   |F     |04983 Stephanie Islands Suite 324|Schneidershire |North Carolina|96156   |USA    |VIP             |2025-03-29       |true     |/Volumes/main/retail_lakehouse/raw_data/customers/customer_data_20250812_080459.csv|2025-08-12 08:22:48.284|customer_bronze|\n+-----------+----------+---------+---------------------+---------------------+-------------+------+---------------------------------+---------------+--------------+--------+-------+----------------+-----------------+---------+-----------------------------------------------------------------------------------+-----------------------+---------------+\n\n\uD83D\uDCCA Customer Total Records: 5,000\n\n\uD83D\uDD0D Product Data Sample:\n+----------+------------+-----------+-----------------+-----------------------------+-----------+----------+-------------+------+----------+-----+----+-----------+---------+---------------------------------------------------------------------------------+-----------------------+--------------+\n|product_id|product_name|category   |sub_category     |brand                        |supplier_id|cost_price|selling_price|weight|dimensions|color|size|launch_date|is_active|file_name                                                                        |processing_time        |bronze_layer  |\n+----------+------------+-----------+-----------------+-----------------------------+-----------+----------+-------------+------+----------+-----+----+-----------+---------+---------------------------------------------------------------------------------+-----------------------+--------------+\n|PRD9069   |Smartphone  |Electronics|Electronics Sub 5|Garcia PLC                   |SUP435     |97.9      |253.48       |5.72  |50x57x73  |Black|XL  |2025-01-27 |true     |/Volumes/main/retail_lakehouse/raw_data/products/product_data_20250812_080459.csv|2025-08-12 08:30:15.245|product_bronze|\n|PRD8343   |Smartphone  |Electronics|Electronics Sub 4|Frey-Wade                    |SUP687     |184.12    |120.26       |14.12 |20x46x74  |Gray |S   |2025-06-20 |true     |/Volumes/main/retail_lakehouse/raw_data/products/product_data_20250812_080459.csv|2025-08-12 08:30:15.245|product_bronze|\n|PRD5844   |Smartphone  |Electronics|Electronics Sub 2|Morales, Hamilton and Russell|SUP822     |36.01     |85.83        |43.34 |46x8x47   |White|XL  |2025-03-18 |true     |/Volumes/main/retail_lakehouse/raw_data/products/product_data_20250812_080459.csv|2025-08-12 08:30:15.245|product_bronze|\n+----------+------------+-----------+-----------------+-----------------------------+-----------+----------+-------------+------+----------+-----+----+-----------+---------+---------------------------------------------------------------------------------+-----------------------+--------------+\n\n\uD83D\uDCCA Product Total Records: 246\n\n\uD83C\uDF89 Bronze Layer Pipeline Summary:\n  Sales Processing: ✅ Success\n  Customer Processing: ✅ Success\n  Product Processing: ✅ Success\n  \uD83D\uDCC1 Data should now be in bronze layer tables!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "catalog_name = \"main\"\n",
    "database_name = \"retail_lakehouse\"\n",
    "volume_name = \"raw_data\"\n",
    "\n",
    "# Paths\n",
    "raw_data_path = f\"/Volumes/{catalog_name}/{database_name}/{volume_name}\"\n",
    "bronze_path = f\"/Volumes/{catalog_name}/{database_name}/bronze\"\n",
    "checkpoints_path = f\"/Volumes/{catalog_name}/{database_name}/checkpoints\"\n",
    "\n",
    "print(\"=== PATH VERIFICATION ===\")\n",
    "print(f\"Raw Data Path: {raw_data_path}\")\n",
    "print(f\"Bronze Layer Path: {bronze_path}\")\n",
    "print(f\"Checkpoints Path: {checkpoints_path}\")\n",
    "\n",
    "# Check if raw data files exist\n",
    "try:\n",
    "    sales_files = dbutils.fs.ls(f\"{raw_data_path}/sales\")\n",
    "    print(f\"Found {len(sales_files)} sales files\")\n",
    "    for file in sales_files[:3]:  # Show first 3 files\n",
    "        print(f\"  - {file.path}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error accessing sales files: {e}\")\n",
    "\n",
    "try:\n",
    "    customer_files = dbutils.fs.ls(f\"{raw_data_path}/customers\")\n",
    "    print(f\" Found {len(customer_files)} customer files\")\n",
    "    for file in customer_files[:3]:  # Show first 3 files\n",
    "        print(f\"  - {file.path}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error accessing customer files: {e}\")\n",
    "\n",
    "try:\n",
    "    product_files = dbutils.fs.ls(f\"{raw_data_path}/products\")\n",
    "    print(f\" Found {len(product_files)} product files\")\n",
    "    for file in product_files[:3]:  # Show first 3 files\n",
    "        print(f\"  - {file.path}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error accessing product files: {e}\")\n",
    "\n",
    "\n",
    "# Create necessary directories\n",
    "try:\n",
    "    dbutils.fs.mkdirs(bronze_path)\n",
    "    dbutils.fs.mkdirs(checkpoints_path)\n",
    "    print(\" Created bronze and checkpoints directories\")\n",
    "except Exception as e:\n",
    "    print(f\"  Warning creating directories: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create database if it doesn't exist\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "spark.sql(f\"USE {database_name}\")\n",
    "print(f\" Using database: {database_name}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define schema for sales data\n",
    "sales_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"transaction_date\", StringType(), True),\n",
    "    StructField(\"transaction_time\", StringType(), True),\n",
    "    StructField(\"store_id\", StringType(), True),\n",
    "    StructField(\"store_name\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"discount_percent\", IntegerType(), True),\n",
    "    StructField(\"discount_amount\", DoubleType(), True),\n",
    "    StructField(\"final_amount\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_segment\", StringType(), True),\n",
    "    StructField(\"sales_person_id\", StringType(), True),\n",
    "    StructField(\"promotion_code\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for customer data\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"date_of_birth\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"address_line1\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip_code\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"customer_segment\", StringType(), True),\n",
    "    StructField(\"registration_date\", StringType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for product data\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"sub_category\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"supplier_id\", StringType(), True),\n",
    "    StructField(\"cost_price\", DoubleType(), True),\n",
    "    StructField(\"selling_price\", DoubleType(), True),\n",
    "    StructField(\"weight\", DoubleType(), True),\n",
    "    StructField(\"dimensions\", StringType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"size\", StringType(), True),\n",
    "    StructField(\"launch_date\", StringType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Auto LoaderS\n",
    "def create_sales_auto_loader():\n",
    "    \"\"\"Create Auto Loader stream for sales data\"\"\"\n",
    "    try:\n",
    "        return (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"csv\")\n",
    "                .option(\"cloudFiles.schemaLocation\", f\"{checkpoints_path}/sales_schema\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"false\")\n",
    "                .schema(sales_schema)\n",
    "                .load(f\"{raw_data_path}/sales\")\n",
    "                .withColumn(\"file_name\", col(\"_metadata.file_path\"))\n",
    "                .withColumn(\"processing_time\", current_timestamp())\n",
    "                .withColumn(\"bronze_layer\", lit(\"sales_bronze\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\" Error creating sales auto loader: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_customer_auto_loader():\n",
    "    \"\"\"Create Auto Loader stream for customer data\"\"\"\n",
    "    try:\n",
    "        return (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"csv\")\n",
    "                .option(\"cloudFiles.schemaLocation\", f\"{checkpoints_path}/customer_schema\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"false\")\n",
    "                .schema(customer_schema)\n",
    "                .load(f\"{raw_data_path}/customers\")\n",
    "                .withColumn(\"file_name\", col(\"_metadata.file_path\"))\n",
    "                .withColumn(\"processing_time\", current_timestamp())\n",
    "                .withColumn(\"bronze_layer\", lit(\"customer_bronze\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\" Error creating customer auto loader: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_product_auto_loader():\n",
    "    \"\"\"Create Auto Loader stream for product data\"\"\"\n",
    "    try:\n",
    "        return (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"csv\")\n",
    "                .option(\"cloudFiles.schemaLocation\", f\"{checkpoints_path}/product_schema\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"false\")\n",
    "                .schema(product_schema)\n",
    "                .load(f\"{raw_data_path}/products\")\n",
    "                .withColumn(\"file_name\", col(\"_metadata.file_path\"))\n",
    "                .withColumn(\"processing_time\", current_timestamp())\n",
    "                .withColumn(\"bronze_layer\", lit(\"product_bronze\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\" Error creating product auto loader: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Process data with detailed logging\n",
    "def process_data_stream(stream_name, create_loader_func, checkpoint_path, output_path):\n",
    "    \"\"\"Process a data stream with detailed error handling\"\"\"\n",
    "    print(f\"\\n=== Processing {stream_name} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Create the stream\n",
    "        stream_df = create_loader_func()\n",
    "        if stream_df is None:\n",
    "            print(f\" Failed to create {stream_name} stream\")\n",
    "            return False\n",
    "            \n",
    "        print(f\"{stream_name} stream created successfully\")\n",
    "        \n",
    "        # Start the stream\n",
    "        query = (stream_df\n",
    "                .writeStream\n",
    "                .format(\"delta\")\n",
    "                .outputMode(\"append\")\n",
    "                .option(\"checkpointLocation\", checkpoint_path)\n",
    "                .option(\"path\", output_path)\n",
    "                .option(\"mergeSchema\", \"true\")  \n",
    "                .trigger(availableNow=True)\n",
    "                .start()\n",
    "        )\n",
    "        \n",
    "        print(f\" {stream_name} stream started, waiting for completion...\")\n",
    "        query.awaitTermination()\n",
    "        \n",
    "        # Check if data was written\n",
    "        try:\n",
    "            count = spark.read.format(\"delta\").load(output_path).count()\n",
    "            print(f\" {stream_name} completed successfully - {count} records written\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\" {stream_name} stream completed but error checking output: {e}\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {stream_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Process all streams\n",
    "sales_success = process_data_stream(\n",
    "    \"Sales Data\",\n",
    "    create_sales_auto_loader,\n",
    "    f\"{checkpoints_path}/sales_bronze\",\n",
    "    f\"{bronze_path}/sales_bronze\"\n",
    ")\n",
    "\n",
    "customer_success = process_data_stream(\n",
    "    \"Customer Data\", \n",
    "    create_customer_auto_loader,\n",
    "    f\"{checkpoints_path}/customer_bronze\",\n",
    "    f\"{bronze_path}/customer_bronze\"\n",
    ")\n",
    "\n",
    "product_success = process_data_stream(\n",
    "    \"Product Data\",\n",
    "    create_product_auto_loader,\n",
    "    f\"{checkpoints_path}/product_bronze\", \n",
    "    f\"{bronze_path}/product_bronze\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create Delta tables\n",
    "print(\"\\n=== Creating Delta Tables ===\")\n",
    "\n",
    "table_configs = [\n",
    "    (\"sales_bronze\", f\"{bronze_path}/sales_bronze\"),\n",
    "    (\"customer_bronze\", f\"{bronze_path}/customer_bronze\"),\n",
    "    (\"product_bronze\", f\"{bronze_path}/product_bronze\")\n",
    "]\n",
    "\n",
    "for table_name, location in table_configs:\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {database_name}.{table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '{location}'\n",
    "        \"\"\")\n",
    "        print(f\" Created/verified table: {database_name}.{table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error creating table {table_name}: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Verify data ingestion using direct Delta path queries\n",
    "print(\"\\n=== DATA INGESTION VERIFICATION ===\")\n",
    "\n",
    "# Check Sales data\n",
    "try:\n",
    "    sales_df = spark.read.format(\"delta\").load(f\"{bronze_path}/sales_bronze\")\n",
    "    sales_count = sales_df.count()\n",
    "    print(f\"Sales Bronze Records: {sales_count:,}\")\n",
    "    if sales_count > 0:\n",
    "        print(\"Sample Sales Data:\")\n",
    "        sales_df.limit(3).show(truncate=False)\n",
    "    else:\n",
    "        print(\"  No sales data found\")\n",
    "except Exception as e:\n",
    "    print(f\" Error reading sales data: {e}\")\n",
    "\n",
    "# Check Customer data\n",
    "try:\n",
    "    customer_df = spark.read.format(\"delta\").load(f\"{bronze_path}/customer_bronze\")\n",
    "    customer_count = customer_df.count()\n",
    "    print(f\" Customer Bronze Records: {customer_count:,}\")\n",
    "    if customer_count > 0:\n",
    "        print(\" Sample Customer Data:\")\n",
    "        customer_df.limit(3).show(truncate=False)\n",
    "    else:\n",
    "        print(\"  No customer data found\")\n",
    "except Exception as e:\n",
    "    print(f\" Error reading customer data: {e}\")\n",
    "\n",
    "# Check Product data\n",
    "try:\n",
    "    product_df = spark.read.format(\"delta\").load(f\"{bronze_path}/product_bronze\")\n",
    "    product_count = product_df.count()\n",
    "    print(f\"  Product Bronze Records: {product_count:,}\")\n",
    "    if product_count > 0:\n",
    "        print(\" Sample Product Data:\")\n",
    "        product_df.limit(3).show(truncate=False)\n",
    "    else:\n",
    "        print(\"  No product data found\")\n",
    "except Exception as e:\n",
    "    print(f\" Error reading product data: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Alternative verification using SQL with direct Delta paths\n",
    "print(\"\\n=== ALTERNATIVE VERIFICATION USING DIRECT DELTA PATHS ===\")\n",
    "\n",
    "verification_queries = [\n",
    "    (\"Sales\", f\"SELECT * FROM delta.`{bronze_path}/sales_bronze`\"),\n",
    "    (\"Customer\", f\"SELECT * FROM delta.`{bronze_path}/customer_bronze`\"),\n",
    "    (\"Product\", f\"SELECT * FROM delta.`{bronze_path}/product_bronze`\")\n",
    "]\n",
    "\n",
    "for name, query in verification_queries:\n",
    "    try:\n",
    "        print(f\"\\n{name} Data Sample:\")\n",
    "        result_df = spark.sql(f\"{query} LIMIT 3\")\n",
    "        result_df.show(truncate=False)\n",
    "        \n",
    "        # Count records\n",
    "        count_df = spark.sql(f\"SELECT COUNT(*) as count FROM delta.`{bronze_path}/sales_bronze`\" if name == \"Sales\" \n",
    "                           else f\"SELECT COUNT(*) as count FROM delta.`{bronze_path}/customer_bronze`\" if name == \"Customer\"\n",
    "                           else f\"SELECT COUNT(*) as count FROM delta.`{bronze_path}/product_bronze`\")\n",
    "        count = count_df.collect()[0]['count']\n",
    "        print(f\" {name} Total Records: {count:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error querying {name} data: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n Bronze Layer Pipeline Summary:\")\n",
    "print(f\"  Sales Processing: {'Success' if sales_success else ' Failed'}\")\n",
    "print(f\"  Customer Processing: {'Success' if customer_success else ' Failed'}\")\n",
    "print(f\"  Product Processing: {'Success' if product_success else 'Failed'}\")\n",
    "print(\"  Data should now be in bronze layer tables!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b9f376e-026a-4d7f-9767-0cf5d3e7d025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6268956471505128,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Bronze_Layer_Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}