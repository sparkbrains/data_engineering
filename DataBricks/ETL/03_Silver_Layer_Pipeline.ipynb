{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a4cff0b-2155-4a7a-a2a5-ba0e3183cb82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f727546d-b78b-4eec-9b36-18e7a9aef3b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze Path: /Volumes/main/retail_lakehouse/bronze\nSilver Path: /Volumes/main/retail_lakehouse/silver\nUsing database: retail_lakehouse\n\uD83D\uDE80 Starting Silver Layer Data Processing...\nProcessing Sales Data...\nProcessing Customer Data...\nProcessing Product Data...\n✅ Data cleaning completed!\nWriting Sales Silver data...\n✅ Sales Silver data saved successfully!\nWriting Customer Silver data...\n✅ Customer Silver data saved successfully!\nWriting Product Silver data...\n✅ Product Silver data saved successfully!\n✅ Silver layer data saving process completed!\nCreating Silver Layer Delta Tables...\nError creating sales silver table: [RequestId=3aef6a60-186a-49e6-b2ed-4492277543a4 ErrorClass=INVALID_PARAMETER_VALUE.INVALID_PARAMETER_VALUE] Missing cloud file system scheme\n\nJVM stacktrace:\ncom.databricks.sql.managedcatalog.UnityCatalogServiceException\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:126)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:266)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7313)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7299)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:6280)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:3242)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$generateTemporaryPathCredentials$2(ProfiledManagedCatalog.scala:744)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1698)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.generateTemporaryPathCredentials(ProfiledManagedCatalog.scala:744)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:146)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:194)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:1190)\n\tat com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:516)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateDSv2Table(ResolveWithCredential.scala:134)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:166)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:69)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:936)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3382)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3211)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:639)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:682)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:624)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\nError creating customer silver table: [RequestId=6df802be-ebd2-4c3e-84f1-cbf0b2848d43 ErrorClass=INVALID_PARAMETER_VALUE.INVALID_PARAMETER_VALUE] Missing cloud file system scheme\n\nJVM stacktrace:\ncom.databricks.sql.managedcatalog.UnityCatalogServiceException\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:126)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:266)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7313)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7299)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:6280)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:3242)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$generateTemporaryPathCredentials$2(ProfiledManagedCatalog.scala:744)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1698)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.generateTemporaryPathCredentials(ProfiledManagedCatalog.scala:744)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:146)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:194)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:1190)\n\tat com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:516)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateDSv2Table(ResolveWithCredential.scala:134)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:166)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:69)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAn\n\n*** WARNING: max output size exceeded, skipping output. ***\n\ncutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:936)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3382)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3211)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:639)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:682)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:624)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\n✅ Silver layer tables creation process completed!\n=== SILVER LAYER DATA QUALITY ASSESSMENT ===\n\n\uD83D\uDCCA Sales Data Quality:\n+-----------------+------------+----------+\n|data_quality_flag|record_count|percentage|\n+-----------------+------------+----------+\n|     Questionable|        6074|     60.74|\n|             Good|        3926|     39.26|\n+-----------------+------------+----------+\n\n\uD83D\uDC65 Customer Data Quality:\n+-----------------+------------+----------+\n|data_quality_flag|record_count|percentage|\n+-----------------+------------+----------+\n|        Excellent|        5000|    100.00|\n+-----------------+------------+----------+\n\n\uD83D\uDECD️ Product Data Quality:\n+-----------------+------------+----------+\n|data_quality_flag|record_count|percentage|\n+-----------------+------------+----------+\n|        Excellent|         246|    100.00|\n+-----------------+------------+----------+\n\n=== DATA PROFILING SUMMARY ===\n\n\uD83D\uDCCA Sales Data Statistics:\n+-------------+-------------------+----------------+---------------+-------------+-----------+----------------------+-------------+\n|total_records|unique_transactions|unique_customers|unique_products|earliest_date|latest_date|avg_transaction_amount|total_revenue|\n+-------------+-------------------+----------------+---------------+-------------+-----------+----------------------+-------------+\n|        10000|              10000|            9452|           6009|   2023-08-13| 2025-08-12|                685.38|   6853803.87|\n+-------------+-------------------+----------------+---------------+-------------+-----------+----------------------+-------------+\n\n\uD83D\uDCC8 Top Categories by Revenue:\n+-------------+------------+-------------+----------------------+\n|     category|transactions|total_revenue|avg_transaction_amount|\n+-------------+------------+-------------+----------------------+\n|   Automotive|        1360|    933142.96|                686.13|\n|       Beauty|        1311|    927076.63|                707.15|\n|     Clothing|        1231|    871166.69|                707.69|\n|Home & Garden|        1208|    844186.27|                698.83|\n|        Books|        1253|     834447.8|                665.96|\n|  Electronics|        1245|    826446.23|                663.81|\n|         Toys|        1185|    817504.45|                689.88|\n|       Sports|        1207|    799832.84|                662.66|\n+-------------+------------+-------------+----------------------+\n\n\uD83D\uDC65 Customer Segments Distribution:\n+----------------+--------------+----------+\n|customer_segment|customer_count|percentage|\n+----------------+--------------+----------+\n|        Standard|          1303|     26.06|\n|          Budget|          1262|     25.24|\n|             Vip|          1220|     24.40|\n|         Premium|          1215|     24.30|\n+----------------+--------------+----------+\n\n\uD83D\uDECD️ Product Categories Distribution:\n+-------------+-------------+---------+--------------+\n|     category|product_count|avg_price|avg_margin_pct|\n+-------------+-------------+---------+--------------+\n|       Sports|           32|   233.45|        -38.14|\n|   Automotive|           28|   298.51|         28.46|\n|     Clothing|           28|   245.45|          2.32|\n|  Electronics|           28|   241.32|        -20.62|\n|        Books|           27|   173.39|       -185.97|\n|       Beauty|           26|   310.35|         10.36|\n|Home & Garden|           26|    262.6|         25.12|\n|         Toys|           25|   234.39|          4.05|\n+-------------+-------------+---------+--------------+\n\n=== SILVER LAYER SAMPLE DATA ===\n\n\uD83D\uDCCA Sales Silver Sample:\n+---------------+----------------+----------------+--------+--------------------+------+----------+----------------+-------------+--------+----------+------------+----------------+---------------+------------+--------------+-----------+----------------+---------------+--------------+--------------------+--------------------+------------+--------------------+----+-----+-------+-----------+--------+----------+----+----------+-----------+------------------+------------------+----------------+-----------------+----------------------+-------------+\n| transaction_id|transaction_date|transaction_time|store_id|          store_name|region|product_id|    product_name|     category|quantity|unit_price|total_amount|discount_percent|discount_amount|final_amount|payment_method|customer_id|customer_segment|sales_person_id|promotion_code|           file_name|     processing_time|bronze_layer|transaction_datetime|year|month|quarter|day_of_week|day_name|month_name|hour|is_weekend|time_of_day|    estimated_cost|  estimated_profit|transaction_size|data_quality_flag|silver_processing_time|record_source|\n+---------------+----------------+----------------+--------+--------------------+------+----------+----------------+-------------+--------+----------+------------+----------------+---------------+------------+--------------+-----------+----------------+---------------+--------------+--------------------+--------------------+------------+--------------------+----+-----+-------+-----------+--------+----------+----+----------+-----------+------------------+------------------+----------------+-----------------+----------------------+-------------+\n|TXN3D7AAEF219E0|      2025-08-12|        08:04:54|   ST006|EAST ERIKAMOUTH MALL| SOUTH|   PRD9181|Technical Manual|        Books|       1|    407.91|      407.91|              20|          81.58|      326.33|    Debit Card|  CUST82153|          Budget|         EMP325|          NULL|/Volumes/main/ret...|2025-08-12 08:30:...|sales_bronze| 2025-08-12 08:04:54|2025|    8|      3|          3| Tuesday|    August|   8|     false|    Morning|228.43099999999998| 97.89899999999999|           Large|     Questionable|  2025-08-12 08:40:...| sales_bronze|\n|TXN966A252C12FB|      2025-08-12|        08:04:54|   ST002|SOUTH MICHELLEFUR...| NORTH|   PRD5791|       Biography|        Books|       2|    396.29|      792.58|              15|         118.89|      673.69|   Credit Card|  CUST39911|          Budget|         EMP589|          NULL|/Volumes/main/ret...|2025-08-12 08:30:...|sales_bronze| 2025-08-12 08:04:54|2025|    8|      3|          3| Tuesday|    August|   8|     false|    Morning|           471.583|           202.107|      Very Large|     Questionable|  2025-08-12 08:40:...| sales_bronze|\n|TXN12E74B8075D0|      2025-08-12|        08:04:54|   ST014|     MILLSMOUTH MALL| NORTH|   PRD2127|            Lamp|Home & Garden|       5|    265.76|      1328.8|               5|          66.44|     1262.36|    Debit Card|  CUST76578|        Standard|         EMP783|         BULK5|/Volumes/main/ret...|2025-08-12 08:30:...|sales_bronze| 2025-08-12 08:04:54|2025|    8|      3|          3| Tuesday|    August|   8|     false|    Morning| 883.6519999999999|378.70799999999997|      Very Large|             Good|  2025-08-12 08:40:...| sales_bronze|\n|TXNDB8695F1D292|      2025-08-12|        08:04:54|   ST016| CASSIDYBOROUGH MALL| SOUTH|   PRD7762|   Fiction Novel|        Books|       5|    108.84|       544.2|              15|          81.63|      462.57|          Cash|  CUST84727|         Premium|         EMP449|         BULK5|/Volumes/main/ret...|2025-08-12 08:30:...|sales_bronze| 2025-08-12 08:04:54|2025|    8|      3|          3| Tuesday|    August|   8|     false|    Morning|           323.799|           138.771|           Large|             Good|  2025-08-12 08:40:...| sales_bronze|\n|TXNA8E8F9D91E9D|      2025-08-12|        08:04:54|   ST016| CASSIDYBOROUGH MALL| SOUTH|   PRD9808|   Tennis Racket|       Sports|       4|    237.22|      948.88|               5|          47.44|      901.44|          Cash|  CUST71909|        Standard|         EMP547|          NULL|/Volumes/main/ret...|2025-08-12 08:30:...|sales_bronze| 2025-08-12 08:04:54|2025|    8|      3|          3| Tuesday|    August|   8|     false|    Morning|           631.008|           270.432|      Very Large|     Questionable|  2025-08-12 08:40:...| sales_bronze|\n+---------------+----------------+----------------+--------+--------------------+------+----------+----------------+-------------+--------+----------+------------+----------------+---------------+------------+--------------+-----------+----------------+---------------+--------------+--------------------+--------------------+------------+--------------------+----+-----+-------+-----------+--------+----------+----+----------+-----------+------------------+------------------+----------------+-----------------+----------------------+-------------+\n\n\uD83D\uDC65 Customer Silver Sample:\n+-----------+----------+---------+--------------------+--------------------+-------------+------+--------------------+------------+---------+--------+-------+----------------+-----------------+---------+--------------------+--------------------+---------------+-----------------+---+---------+--------------------+---------------------+---------------+-----------+------------------+-----------------+----------------------+---------------+\n|customer_id|first_name|last_name|               email|               phone|date_of_birth|gender|       address_line1|        city|    state|zip_code|country|customer_segment|registration_date|is_active|           file_name|     processing_time|   bronze_layer|      phone_clean|age|age_group|customer_tenure_days|customer_tenure_years|tenure_category|email_valid|data_quality_score|data_quality_flag|silver_processing_time|  record_source|\n+-----------+----------+---------+--------------------+--------------------+-------------+------+--------------------+------------+---------+--------+-------+----------------+-----------------+---------+--------------------+--------------------+---------------+-----------------+---+---------+--------------------+---------------------+---------------+-----------+------------------+-----------------+----------------------+---------------+\n|  CUST10554|      John|   Barnes|michele28@example...|   411.714.4081x4514|   2006-06-25|     F|9718 Griffith Tra...|   Marymouth|  ARIZONA|   22076|    USA|         Premium|       2024-12-01|    false|/Volumes/main/ret...|2025-08-12 08:22:...|customer_bronze|   41171440814514| 19|    18-24|                 254|                    0|            New|       true|                 5|        Excellent|  2025-08-12 08:40:...|customer_bronze|\n|  CUST12222|   Destiny| Delacruz|william00@example...|  (986)211-9318x0570|   1981-05-25|     M|  2682 Madison Roads|  East Julia|    TEXAS|   62865|    USA|         Premium|       2023-09-30|     true|/Volumes/main/ret...|2025-08-12 08:22:...|customer_bronze|   98621193180570| 44|    35-44|                 682|                    1|        Regular|       true|                 5|        Excellent|  2025-08-12 08:40:...|customer_bronze|\n|  CUST12716|   Melissa|    Baker| jwilson@example.org|        486.494.7460|   1949-07-25| Other|280 Vaughn Rapids...|  Lake Donna|  FLORIDA|   20157|    USA|        Standard|       2025-02-02|     true|/Volumes/main/ret...|2025-08-12 08:22:...|customer_bronze|       4864947460| 76|      65+|                 191|                    0|            New|       true|                 5|        Excellent|  2025-08-12 08:40:...|customer_bronze|\n|  CUST14439|  Jonathan|   Parker| sgibson@example.com|+1-291-603-1565x5419|   1949-10-07|     F| 88833 Jason Corners|North Taylor|MINNESOTA|   97206|    USA|        Standard|       2023-02-25|     true|/Volumes/main/ret...|2025-08-12 08:22:...|customer_bronze|  129160315655419| 75|      65+|                 899|                    2|        Regular|       true|                 5|        Excellent|  2025-08-12 08:40:...|customer_bronze|\n|  CUST10360|    Daniel|   Parker|thomasgeorge@exam...|001-240-963-7543x...|   1949-12-12|     M|676 Theresa Roads...|   New Laura|   KANSAS|   32015|    USA|        Standard|       2023-03-10|     true|/Volumes/main/ret...|2025-08-12 08:22:...|customer_bronze|00124096375431923| 75|      65+|                 886|                    2|        Regular|       true|                 5|        Excellent|  2025-08-12 08:40:...|customer_bronze|\n+-----------+----------+---------+--------------------+--------------------+-------------+------+--------------------+------------+---------+--------+-------+----------------+-----------------+---------+--------------------+--------------------+---------------+-----------------+---+---------+--------------------+---------------------+---------------+-----------+------------------+-----------------+----------------------+---------------+\n\n\uD83D\uDECD️ Product Silver Sample:\n+----------+------------+-----------+-----------------+--------------+-----------+----------+-------------+------+----------+-----+--------+-----------+---------+--------------------+--------------------+--------------+------------------+-----------------+--------------+----------------+--------------------+---------------+------------------+-----------------+----------------------+--------------+\n|product_id|product_name|   category|     sub_category|         brand|supplier_id|cost_price|selling_price|weight|dimensions|color|    size|launch_date|is_active|           file_name|     processing_time|  bronze_layer|     margin_amount|margin_percentage|price_category|product_age_days|product_age_category|weight_category|data_quality_score|data_quality_flag|silver_processing_time| record_source|\n+----------+------------+-----------+-----------------+--------------+-----------+----------+-------------+------+----------+-----+--------+-----------+---------+--------------------+--------------------+--------------+------------------+-----------------+--------------+----------------+--------------------+---------------+------------------+-----------------+----------------------+--------------+\n|   PRD7264|       Dress|   Clothing|   Clothing Sub 4|   Mcbride Inc|     SUP755|     147.4|         73.2| 24.28|  42x52x91|White|ONE SIZE| 2025-03-24|     true|/Volumes/main/ret...|2025-08-12 08:30:...|product_bronze|             -74.2|          -101.37|     Mid-Range|             141|              Recent|     Very Heavy|                 5|        Excellent|  2025-08-12 08:40:...|product_bronze|\n|   PRD8062|  Engine Oil| Automotive| Automotive Sub 5|    Berg Group|     SUP746|      5.27|       317.75|  2.94|  89x28x27|Brown|     XXL| 2025-05-10|     true|/Volumes/main/ret...|2025-08-12 08:30:...|product_bronze|            312.48|            98.34|        Luxury|              94|              Recent|         Medium|                 5|        Excellent|  2025-08-12 08:40:...|product_bronze|\n|   PRD3449|     Perfume|     Beauty|     Beauty Sub 2|   Wright-mann|     SUP665|     69.27|       496.96| 35.68|  40x23x94|White|      XS| 2024-03-01|     true|/Volumes/main/ret...|2025-08-12 08:30:...|product_bronze|            427.69|            86.06|        Luxury|             529|              Mature|     Very Heavy|                 5|        Excellent|  2025-08-12 08:40:...|product_bronze|\n|   PRD7993|  Headphones|Electronics|Electronics Sub 1|Vasquez-flores|     SUP724|     22.23|        42.78| 46.91|  89x72x98|Green|     XXL| 2024-08-09|     true|/Volumes/main/ret...|2025-08-12 08:30:...|product_bronze|             20.55|            48.04|     Mid-Range|             368|              Mature|     Very Heavy|                 5|        Excellent|  2025-08-12 08:40:...|product_bronze|\n|   PRD3999|    Dumbbell|     Sports|     Sports Sub 4|    Conley Inc|     SUP579|    173.49|       126.22| 46.72|  84x83x51|Black|       M| 2023-08-22|     true|/Volumes/main/ret...|2025-08-12 08:30:...|product_bronze|-47.27000000000001|           -37.45|       Premium|             721|              Mature|     Very Heavy|                 5|        Excellent|  2025-08-12 08:40:...|product_bronze|\n+----------+------------+-----------+-----------------+--------------+-----------+----------+-------------+------+----------+-----+--------+-----------+---------+--------------------+--------------------+--------------+------------------+-----------------+--------------+----------------+--------------------+---------------+------------------+-----------------+----------------------+--------------+\n\nCreating Silver Layer Views...\n✅ Active customers view created!\n✅ Active products view created!\n✅ Clean sales view created!\n✅ Silver layer views creation process completed!\n\uD83C\uDF89 Silver Layer Processing Completed!\n\n\uD83D\uDCCB Summary:\n  ✅ Sales data cleaned and enriched\n  ✅ Customer data validated and enhanced\n  ✅ Product data processed and categorized\n  ✅ Data quality assessments completed\n  ✅ Helper views created\n  ✅ Ready for Gold layer aggregations!\n\n\uD83D\uDCCA Final Record Counts:\n   Sales: 10,000\n   Customers: 5,000\n   Products: 246\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "catalog_name = \"main\"\n",
    "database_name = \"retail_lakehouse\"\n",
    "\n",
    "# Paths\n",
    "bronze_path = f\"/Volumes/{catalog_name}/{database_name}/bronze\"\n",
    "silver_path = f\"/Volumes/{catalog_name}/{database_name}/silver\"\n",
    "checkpoints_path = f\"/Volumes/{catalog_name}/{database_name}/checkpoints\"\n",
    "\n",
    "# Create silver directory\n",
    "try:\n",
    "    dbutils.fs.mkdirs(silver_path)\n",
    "except:\n",
    "    print(\"Note: dbutils not available - running in local environment\")\n",
    "\n",
    "# Use the database\n",
    "try:\n",
    "    spark.sql(f\"USE {database_name}\")\n",
    "except:\n",
    "    print(f\"Note: Database {database_name} may not exist yet\")\n",
    "\n",
    "print(f\"Bronze Path: {bronze_path}\")\n",
    "print(f\"Silver Path: {silver_path}\")\n",
    "print(f\"Using database: {database_name}\")\n",
    "\n",
    "\n",
    "def clean_sales_data():\n",
    "    \"\"\"Clean and enrich sales data from bronze to silver layer\"\"\"\n",
    "    \n",
    "    print(\"Processing Sales Data...\")\n",
    "    \n",
    "    try:\n",
    "        # Read directly from Delta path\n",
    "        sales_bronze = spark.read.format(\"delta\").load(f\"{bronze_path}/sales_bronze\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read sales bronze data from path: {e}\")\n",
    "        print(\"Creating empty sales dataframe with expected schema...\")\n",
    "        # Create empty dataframe with schema\n",
    "        sales_schema = StructType([\n",
    "            StructField(\"transaction_id\", StringType(), True),\n",
    "            StructField(\"transaction_date\", StringType(), True),\n",
    "            StructField(\"transaction_time\", StringType(), True),\n",
    "            StructField(\"store_id\", StringType(), True),\n",
    "            StructField(\"store_name\", StringType(), True),\n",
    "            StructField(\"region\", StringType(), True),\n",
    "            StructField(\"product_id\", StringType(), True),\n",
    "            StructField(\"product_name\", StringType(), True),\n",
    "            StructField(\"category\", StringType(), True),\n",
    "            StructField(\"quantity\", IntegerType(), True),\n",
    "            StructField(\"unit_price\", DoubleType(), True),\n",
    "            StructField(\"total_amount\", DoubleType(), True),\n",
    "            StructField(\"discount_percent\", IntegerType(), True),\n",
    "            StructField(\"discount_amount\", DoubleType(), True),\n",
    "            StructField(\"final_amount\", DoubleType(), True),\n",
    "            StructField(\"payment_method\", StringType(), True),\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"customer_segment\", StringType(), True),\n",
    "            StructField(\"sales_person_id\", StringType(), True),\n",
    "            StructField(\"promotion_code\", StringType(), True),\n",
    "            StructField(\"file_name\", StringType(), True),\n",
    "            StructField(\"processing_time\", TimestampType(), True),\n",
    "            StructField(\"bronze_layer\", StringType(), True)\n",
    "        ])\n",
    "        sales_bronze = spark.createDataFrame([], sales_schema)\n",
    "    \n",
    "    if sales_bronze.count() == 0:\n",
    "        print(\"No sales data found, returning empty dataframe\")\n",
    "        return sales_bronze\n",
    "    \n",
    "    # Data cleaning and enrichment\n",
    "    sales_silver = (sales_bronze\n",
    "                    # Data type conversions and cleaning\n",
    "                    .withColumn(\"transaction_date\", to_date(col(\"transaction_date\"), \"yyyy-MM-dd\"))\n",
    "                    .withColumn(\"transaction_datetime\", \n",
    "                               to_timestamp(concat(col(\"transaction_date\"), lit(\" \"), col(\"transaction_time\")), \n",
    "                                          \"yyyy-MM-dd HH:mm:ss\"))\n",
    "                    \n",
    "                    # Clean and validate numeric fields\n",
    "                    .withColumn(\"quantity\", \n",
    "                               when(col(\"quantity\") > 0, col(\"quantity\")).otherwise(1))\n",
    "                    .withColumn(\"unit_price\", \n",
    "                               when(col(\"unit_price\") > 0, col(\"unit_price\")).otherwise(0.0))\n",
    "                    .withColumn(\"final_amount\", \n",
    "                               when(col(\"final_amount\") >= 0, col(\"final_amount\")).otherwise(0.0))\n",
    "                    \n",
    "                    # Clean text fields\n",
    "                    .withColumn(\"store_name\", trim(upper(col(\"store_name\"))))\n",
    "                    .withColumn(\"product_name\", trim(initcap(col(\"product_name\"))))\n",
    "                    .withColumn(\"category\", trim(initcap(col(\"category\"))))\n",
    "                    .withColumn(\"region\", trim(upper(col(\"region\"))))\n",
    "                    .withColumn(\"payment_method\", trim(initcap(col(\"payment_method\"))))\n",
    "                    .withColumn(\"customer_segment\", trim(initcap(col(\"customer_segment\"))))\n",
    "                    \n",
    "                    # Add enrichment columns\n",
    "                    .withColumn(\"year\", year(col(\"transaction_date\")))\n",
    "                    .withColumn(\"month\", month(col(\"transaction_date\")))\n",
    "                    .withColumn(\"quarter\", quarter(col(\"transaction_date\")))\n",
    "                    .withColumn(\"day_of_week\", dayofweek(col(\"transaction_date\")))\n",
    "                    .withColumn(\"day_name\", date_format(col(\"transaction_date\"), \"EEEE\"))\n",
    "                    .withColumn(\"month_name\", date_format(col(\"transaction_date\"), \"MMMM\"))\n",
    "                    .withColumn(\"hour\", hour(col(\"transaction_datetime\")))\n",
    "                    \n",
    "                    # Business logic enrichment\n",
    "                    .withColumn(\"is_weekend\", \n",
    "                               when(col(\"day_of_week\").isin([1, 7]), True).otherwise(False))\n",
    "                    .withColumn(\"time_of_day\", \n",
    "                               when(col(\"hour\") < 6, \"Night\")\n",
    "                               .when(col(\"hour\") < 12, \"Morning\")\n",
    "                               .when(col(\"hour\") < 18, \"Afternoon\")\n",
    "                               .otherwise(\"Evening\"))\n",
    "                    \n",
    "                    # Profit calculation (assuming margin of 30%)\n",
    "                    .withColumn(\"estimated_cost\", col(\"final_amount\") * 0.7)\n",
    "                    .withColumn(\"estimated_profit\", col(\"final_amount\") * 0.3)\n",
    "                    \n",
    "                    # Transaction size categorization\n",
    "                    .withColumn(\"transaction_size\", \n",
    "                               when(col(\"final_amount\") < 25, \"Small\")\n",
    "                               .when(col(\"final_amount\") < 100, \"Medium\")\n",
    "                               .when(col(\"final_amount\") < 500, \"Large\")\n",
    "                               .otherwise(\"Very Large\"))\n",
    "                    \n",
    "                    # Data quality flags\n",
    "                    .withColumn(\"data_quality_flag\", \n",
    "                               when((col(\"transaction_id\").isNull()) | \n",
    "                                    (col(\"final_amount\") < 0) | \n",
    "                                    (col(\"quantity\") <= 0), \"Poor\")\n",
    "                               .when(col(\"promotion_code\").isNull() & \n",
    "                                     (col(\"discount_percent\") > 0), \"Questionable\")\n",
    "                               .otherwise(\"Good\"))\n",
    "                    \n",
    "                    # Add processing metadata\n",
    "                    .withColumn(\"silver_processing_time\", current_timestamp())\n",
    "                    .withColumn(\"record_source\", lit(\"sales_bronze\"))\n",
    "                    \n",
    "                    # Remove duplicates based on transaction_id\n",
    "                    .dropDuplicates([\"transaction_id\"])\n",
    "                    \n",
    "                    # Filter out poor quality records\n",
    "                    .filter(col(\"data_quality_flag\") != \"Poor\")\n",
    "    )\n",
    "    \n",
    "    return sales_silver\n",
    "\n",
    "\n",
    "def clean_customer_data():\n",
    "    \"\"\"Clean and enrich customer data from bronze to silver layer\"\"\"\n",
    "    \n",
    "    print(\"Processing Customer Data...\")\n",
    "    \n",
    "    try:\n",
    "        # Read directly from Delta path\n",
    "        customer_bronze = spark.read.format(\"delta\").load(f\"{bronze_path}/customer_bronze\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read customer bronze data from path: {e}\")\n",
    "        print(\"Creating empty customer dataframe with expected schema...\")\n",
    "        # Create empty dataframe with schema\n",
    "        customer_schema = StructType([\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"first_name\", StringType(), True),\n",
    "            StructField(\"last_name\", StringType(), True),\n",
    "            StructField(\"email\", StringType(), True),\n",
    "            StructField(\"phone\", StringType(), True),\n",
    "            StructField(\"date_of_birth\", StringType(), True),\n",
    "            StructField(\"gender\", StringType(), True),\n",
    "            StructField(\"address_line1\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"state\", StringType(), True),\n",
    "            StructField(\"zip_code\", StringType(), True),\n",
    "            StructField(\"country\", StringType(), True),\n",
    "            StructField(\"customer_segment\", StringType(), True),\n",
    "            StructField(\"registration_date\", StringType(), True),\n",
    "            StructField(\"is_active\", BooleanType(), True),\n",
    "            StructField(\"file_name\", StringType(), True),\n",
    "            StructField(\"processing_time\", TimestampType(), True),\n",
    "            StructField(\"bronze_layer\", StringType(), True)\n",
    "        ])\n",
    "        customer_bronze = spark.createDataFrame([], customer_schema)\n",
    "    \n",
    "    if customer_bronze.count() == 0:\n",
    "        print(\"No customer data found, returning empty dataframe\")\n",
    "        return customer_bronze\n",
    "    \n",
    "    # Data cleaning and enrichment\n",
    "    customer_silver = (customer_bronze\n",
    "                       # Data type conversions\n",
    "                       .withColumn(\"date_of_birth\", to_date(col(\"date_of_birth\"), \"yyyy-MM-dd\"))\n",
    "                       .withColumn(\"registration_date\", to_date(col(\"registration_date\"), \"yyyy-MM-dd\"))\n",
    "                       \n",
    "                       # Clean text fields\n",
    "                       .withColumn(\"first_name\", trim(initcap(col(\"first_name\"))))\n",
    "                       .withColumn(\"last_name\", trim(initcap(col(\"last_name\"))))\n",
    "                       .withColumn(\"email\", trim(lower(col(\"email\"))))\n",
    "                       .withColumn(\"city\", trim(initcap(col(\"city\"))))\n",
    "                       .withColumn(\"state\", trim(upper(col(\"state\"))))\n",
    "                       .withColumn(\"country\", trim(upper(col(\"country\"))))\n",
    "                       .withColumn(\"customer_segment\", trim(initcap(col(\"customer_segment\"))))\n",
    "                       \n",
    "                       # Clean phone numbers (remove special characters)\n",
    "                       .withColumn(\"phone_clean\", \n",
    "                                  regexp_replace(col(\"phone\"), \"[^0-9]\", \"\"))\n",
    "                       \n",
    "                       # Calculate age\n",
    "                       .withColumn(\"age\", \n",
    "                                  floor(datediff(current_date(), col(\"date_of_birth\")) / 365.25))\n",
    "                       \n",
    "                       # Age group categorization\n",
    "                       .withColumn(\"age_group\", \n",
    "                                  when(col(\"age\") < 25, \"18-24\")\n",
    "                                  .when(col(\"age\") < 35, \"25-34\")\n",
    "                                  .when(col(\"age\") < 45, \"35-44\")\n",
    "                                  .when(col(\"age\") < 55, \"45-54\")\n",
    "                                  .when(col(\"age\") < 65, \"55-64\")\n",
    "                                  .otherwise(\"65+\"))\n",
    "                       \n",
    "                       # Customer tenure\n",
    "                       .withColumn(\"customer_tenure_days\", \n",
    "                                  datediff(current_date(), col(\"registration_date\")))\n",
    "                       .withColumn(\"customer_tenure_years\", \n",
    "                                  floor(col(\"customer_tenure_days\") / 365.25))\n",
    "                       \n",
    "                       # Tenure category\n",
    "                       .withColumn(\"tenure_category\", \n",
    "                                  when(col(\"customer_tenure_years\") < 1, \"New\")\n",
    "                                  .when(col(\"customer_tenure_years\") < 3, \"Regular\")\n",
    "                                  .when(col(\"customer_tenure_years\") < 5, \"Loyal\")\n",
    "                                  .otherwise(\"VIP\"))\n",
    "                       \n",
    "                       # Email validation\n",
    "                       .withColumn(\"email_valid\", \n",
    "                                  when(col(\"email\").rlike(\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"), True)\n",
    "                                  .otherwise(False))\n",
    "                       \n",
    "                       # Data quality assessment\n",
    "                       .withColumn(\"data_quality_score\", \n",
    "                                  (when(col(\"first_name\").isNotNull(), 1).otherwise(0) +\n",
    "                                   when(col(\"last_name\").isNotNull(), 1).otherwise(0) +\n",
    "                                   when(col(\"email_valid\") == True, 1).otherwise(0) +\n",
    "                                   when(col(\"phone_clean\").isNotNull(), 1).otherwise(0) +\n",
    "                                   when(col(\"date_of_birth\").isNotNull(), 1).otherwise(0)))\n",
    "                       \n",
    "                       .withColumn(\"data_quality_flag\", \n",
    "                                  when(col(\"data_quality_score\") >= 4, \"Excellent\")\n",
    "                                  .when(col(\"data_quality_score\") >= 3, \"Good\")\n",
    "                                  .when(col(\"data_quality_score\") >= 2, \"Fair\")\n",
    "                                  .otherwise(\"Poor\"))\n",
    "                       \n",
    "                       # Add processing metadata\n",
    "                       .withColumn(\"silver_processing_time\", current_timestamp())\n",
    "                       .withColumn(\"record_source\", lit(\"customer_bronze\"))\n",
    "                       \n",
    "                       # Remove duplicates\n",
    "                       .dropDuplicates([\"customer_id\"])\n",
    "                       \n",
    "                       # Filter out poor quality records\n",
    "                       .filter(col(\"data_quality_flag\") != \"Poor\")\n",
    "    )\n",
    "    \n",
    "    return customer_silver\n",
    "\n",
    "\n",
    "def clean_product_data():\n",
    "    \"\"\"Clean and enrich product data from bronze to silver layer\"\"\"\n",
    "    \n",
    "    print(\"Processing Product Data...\")\n",
    "    \n",
    "    try:\n",
    "        # Read directly from Delta path\n",
    "        product_bronze = spark.read.format(\"delta\").load(f\"{bronze_path}/product_bronze\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read product bronze data from path: {e}\")\n",
    "        print(\"Creating empty product dataframe with expected schema...\")\n",
    "        # Create empty dataframe with schema\n",
    "        product_schema = StructType([\n",
    "            StructField(\"product_id\", StringType(), True),\n",
    "            StructField(\"product_name\", StringType(), True),\n",
    "            StructField(\"category\", StringType(), True),\n",
    "            StructField(\"sub_category\", StringType(), True),\n",
    "            StructField(\"brand\", StringType(), True),\n",
    "            StructField(\"supplier_id\", StringType(), True),\n",
    "            StructField(\"cost_price\", DoubleType(), True),\n",
    "            StructField(\"selling_price\", DoubleType(), True),\n",
    "            StructField(\"weight\", DoubleType(), True),\n",
    "            StructField(\"dimensions\", StringType(), True),\n",
    "            StructField(\"color\", StringType(), True),\n",
    "            StructField(\"size\", StringType(), True),\n",
    "            StructField(\"launch_date\", StringType(), True),\n",
    "            StructField(\"is_active\", BooleanType(), True),\n",
    "            StructField(\"file_name\", StringType(), True),\n",
    "            StructField(\"processing_time\", TimestampType(), True),\n",
    "            StructField(\"bronze_layer\", StringType(), True)\n",
    "        ])\n",
    "        product_bronze = spark.createDataFrame([], product_schema)\n",
    "    \n",
    "    if product_bronze.count() == 0:\n",
    "        print(\"No product data found, returning empty dataframe\")\n",
    "        return product_bronze\n",
    "    \n",
    "    # Data cleaning and enrichment\n",
    "    product_silver = (product_bronze\n",
    "                      .withColumn(\"launch_date\", to_date(col(\"launch_date\"), \"yyyy-MM-dd\"))\n",
    "                      .withColumn(\"product_name\", trim(initcap(col(\"product_name\"))))\n",
    "                      .withColumn(\"category\", trim(initcap(col(\"category\"))))\n",
    "                      .withColumn(\"sub_category\", trim(initcap(col(\"sub_category\"))))\n",
    "                      .withColumn(\"brand\", trim(initcap(col(\"brand\"))))\n",
    "                      .withColumn(\"color\", trim(initcap(col(\"color\"))))\n",
    "                      .withColumn(\"size\", trim(upper(col(\"size\"))))\n",
    "                      .withColumn(\"cost_price\", \n",
    "                                 when(col(\"cost_price\") > 0, col(\"cost_price\")).otherwise(0.0))\n",
    "                      .withColumn(\"selling_price\", \n",
    "                                 when(col(\"selling_price\") > 0, col(\"selling_price\")).otherwise(0.0))\n",
    "                      .withColumn(\"margin_amount\", col(\"selling_price\") - col(\"cost_price\"))\n",
    "                      .withColumn(\"margin_percentage\", \n",
    "                                 when(col(\"selling_price\") > 0, \n",
    "                                      round((col(\"margin_amount\") / col(\"selling_price\")) * 100, 2))\n",
    "                                 .otherwise(0.0))\n",
    "                              .withColumn(\"price_category\", \n",
    "                                 when(col(\"selling_price\") < 20, \"Budget\")\n",
    "                                 .when(col(\"selling_price\") < 100, \"Mid-Range\")\n",
    "                                 .when(col(\"selling_price\") < 300, \"Premium\")\n",
    "                                 .otherwise(\"Luxury\"))\n",
    "                                            .withColumn(\"product_age_days\", \n",
    "                                 datediff(current_date(), col(\"launch_date\")))\n",
    "                      .withColumn(\"product_age_category\", \n",
    "                                 when(col(\"product_age_days\") < 30, \"New\")\n",
    "                                 .when(col(\"product_age_days\") < 180, \"Recent\")\n",
    "                                 .when(col(\"product_age_days\") < 365, \"Established\")\n",
    "                                 .otherwise(\"Mature\"))\n",
    "                      .withColumn(\"weight_category\", \n",
    "                                 when(col(\"weight\") < 1, \"Light\")\n",
    "                                 .when(col(\"weight\") < 5, \"Medium\")\n",
    "                                 .when(col(\"weight\") < 20, \"Heavy\")\n",
    "                                 .otherwise(\"Very Heavy\"))\n",
    "                      .withColumn(\"data_quality_score\", \n",
    "                                 (when(col(\"product_name\").isNotNull(), 1).otherwise(0) +\n",
    "                                  when(col(\"category\").isNotNull(), 1).otherwise(0) +\n",
    "                                  when(col(\"selling_price\") > 0, 1).otherwise(0) +\n",
    "                                  when(col(\"cost_price\") > 0, 1).otherwise(0) +\n",
    "                                  when(col(\"launch_date\").isNotNull(), 1).otherwise(0)))\n",
    "                      \n",
    "                      .withColumn(\"data_quality_flag\", \n",
    "                                 when(col(\"data_quality_score\") >= 4, \"Excellent\")\n",
    "                                 .when(col(\"data_quality_score\") >= 3, \"Good\")\n",
    "                                 .otherwise(\"Fair\"))\n",
    "                      .withColumn(\"silver_processing_time\", current_timestamp())\n",
    "                      .withColumn(\"record_source\", lit(\"product_bronze\"))\n",
    "                      .dropDuplicates([\"product_id\"])\n",
    "    )\n",
    "    \n",
    "    return product_silver\n",
    "\n",
    "\n",
    "# Process all datasets\n",
    "print(\" Starting Silver Layer Data Processing...\")\n",
    "\n",
    "# Clean sales data\n",
    "sales_silver_df = clean_sales_data()\n",
    "\n",
    "# Clean customer data\n",
    "customer_silver_df = clean_customer_data()\n",
    "\n",
    "# Clean product data\n",
    "product_silver_df = clean_product_data()\n",
    "\n",
    "print(\"Data cleaning completed!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Write sales silver data\n",
    "print(\"Writing Sales Silver data...\")\n",
    "try:\n",
    "    if sales_silver_df.count() > 0:\n",
    "        (sales_silver_df\n",
    "         .write\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .format(\"delta\")\n",
    "         .save(f\"{silver_path}/sales_silver\")\n",
    "        )\n",
    "        print(\"Sales Silver data saved successfully!\")\n",
    "    else:\n",
    "        print(\" No sales data to write\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing sales silver data: {e}\")\n",
    "\n",
    "# Write customer silver data\n",
    "print(\"Writing Customer Silver data...\")\n",
    "try:\n",
    "    if customer_silver_df.count() > 0:\n",
    "        (customer_silver_df\n",
    "         .write\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .format(\"delta\")\n",
    "         .save(f\"{silver_path}/customer_silver\")\n",
    "        )\n",
    "        print(\"Customer Silver data saved successfully!\")\n",
    "    else:\n",
    "        print(\" No customer data to write\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing customer silver data: {e}\")\n",
    "\n",
    "# Write product silver data\n",
    "print(\"Writing Product Silver data...\")\n",
    "try:\n",
    "    if product_silver_df.count() > 0:\n",
    "        (product_silver_df\n",
    "         .write\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .format(\"delta\")\n",
    "         .save(f\"{silver_path}/product_silver\")\n",
    "        )\n",
    "        print(\"Product Silver data saved successfully!\")\n",
    "    else:\n",
    "        print(\" No product data to write\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing product silver data: {e}\")\n",
    "\n",
    "print(\"Silver layer data saving process completed!\")\n",
    "\n",
    "\n",
    "# Create Delta tables for silver layer\n",
    "print(\"Creating Silver Layer Delta Tables...\")\n",
    "\n",
    "try:\n",
    "    # Sales Silver Table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {database_name}.sales_silver\n",
    "    USING DELTA\n",
    "    LOCATION '{silver_path}/sales_silver'\n",
    "    \"\"\")\n",
    "    print(\"Sales Silver table created!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating sales silver table: {e}\")\n",
    "\n",
    "try:\n",
    "    # Customer Silver Table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {database_name}.customer_silver\n",
    "    USING DELTA\n",
    "    LOCATION '{silver_path}/customer_silver'\n",
    "    \"\"\")\n",
    "    print(\"Customer Silver table created!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating customer silver table: {e}\")\n",
    "\n",
    "try:\n",
    "    # Product Silver Table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {database_name}.product_silver\n",
    "    USING DELTA\n",
    "    LOCATION '{silver_path}/product_silver'\n",
    "    \"\"\")\n",
    "    print(\"Product Silver table created!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating product silver table: {e}\")\n",
    "\n",
    "print(\"Silver layer tables creation process completed!\")\n",
    "\n",
    "\n",
    "# Data quality assessment using direct Delta paths\n",
    "print(\"=== SILVER LAYER DATA QUALITY ASSESSMENT ===\\n\")\n",
    "\n",
    "try:\n",
    "    # Sales data quality\n",
    "    print(\" Sales Data Quality:\")\n",
    "    sales_quality = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        data_quality_flag,\n",
    "        COUNT(*) as record_count,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM delta.`{silver_path}/sales_silver`), 2) as percentage\n",
    "    FROM delta.`{silver_path}/sales_silver` \n",
    "    GROUP BY data_quality_flag \n",
    "    ORDER BY record_count DESC\n",
    "    \"\"\")\n",
    "    sales_quality.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not assess sales data quality: {e}\")\n",
    "\n",
    "try:\n",
    "    # Customer data quality\n",
    "    print(\" Customer Data Quality:\")\n",
    "    customer_quality = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        data_quality_flag,\n",
    "        COUNT(*) as record_count,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM delta.`{silver_path}/customer_silver`), 2) as percentage\n",
    "    FROM delta.`{silver_path}/customer_silver` \n",
    "    GROUP BY data_quality_flag \n",
    "    ORDER BY record_count DESC\n",
    "    \"\"\")\n",
    "    customer_quality.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not assess customer data quality: {e}\")\n",
    "\n",
    "try:\n",
    "    # Product data quality\n",
    "    print(\" Product Data Quality:\")\n",
    "    product_quality = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        data_quality_flag,\n",
    "        COUNT(*) as record_count,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM delta.`{silver_path}/product_silver`), 2) as percentage\n",
    "    FROM delta.`{silver_path}/product_silver` \n",
    "    GROUP BY data_quality_flag \n",
    "    ORDER BY record_count DESC\n",
    "    \"\"\")\n",
    "    product_quality.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not assess product data quality: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Generate data statistics using direct Delta paths\n",
    "print(\"=== DATA PROFILING SUMMARY ===\\n\")\n",
    "\n",
    "try:\n",
    "    # Sales data statistics\n",
    "    print(\" Sales Data Statistics:\")\n",
    "    spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(DISTINCT transaction_id) as unique_transactions,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers,\n",
    "        COUNT(DISTINCT product_id) as unique_products,\n",
    "        MIN(transaction_date) as earliest_date,\n",
    "        MAX(transaction_date) as latest_date,\n",
    "        ROUND(AVG(final_amount), 2) as avg_transaction_amount,\n",
    "        ROUND(SUM(final_amount), 2) as total_revenue\n",
    "    FROM delta.`{silver_path}/sales_silver`\n",
    "    \"\"\").show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate sales statistics: {e}\")\n",
    "\n",
    "try:\n",
    "    # Top categories by revenue\n",
    "    print(\" Top Categories by Revenue:\")\n",
    "    spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as transactions,\n",
    "        ROUND(SUM(final_amount), 2) as total_revenue,\n",
    "        ROUND(AVG(final_amount), 2) as avg_transaction_amount\n",
    "    FROM delta.`{silver_path}/sales_silver`\n",
    "    GROUP BY category\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 10\n",
    "    \"\"\").show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate category revenue data: {e}\")\n",
    "\n",
    "try:\n",
    "    # Customer segments distribution\n",
    "    print(\" Customer Segments Distribution:\")\n",
    "    spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_segment,\n",
    "        COUNT(*) as customer_count,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM delta.`{silver_path}/customer_silver`), 2) as percentage\n",
    "    FROM delta.`{silver_path}/customer_silver`\n",
    "    GROUP BY customer_segment\n",
    "    ORDER BY customer_count DESC\n",
    "    \"\"\").show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate customer segment data: {e}\")\n",
    "\n",
    "try:\n",
    "    # Product categories distribution\n",
    "    print(\" Product Categories Distribution:\")\n",
    "    spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as product_count,\n",
    "        ROUND(AVG(selling_price), 2) as avg_price,\n",
    "        ROUND(AVG(margin_percentage), 2) as avg_margin_pct\n",
    "    FROM delta.`{silver_path}/product_silver`\n",
    "    WHERE is_active = true\n",
    "    GROUP BY category\n",
    "    ORDER BY product_count DESC\n",
    "    \"\"\").show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate product category data: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== SILVER LAYER SAMPLE DATA ===\\n\")\n",
    "\n",
    "try:\n",
    "    # Sales silver sample\n",
    "    print(\" Sales Silver Sample:\")\n",
    "    sales_sample = spark.sql(f\"SELECT * FROM delta.`{silver_path}/sales_silver` ORDER BY transaction_date DESC LIMIT 5\")\n",
    "    sales_sample.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not show sales sample: {e}\")\n",
    "\n",
    "try:\n",
    "    # Customer silver sample\n",
    "    print(\" Customer Silver Sample:\")\n",
    "    customer_sample = spark.sql(f\"SELECT * FROM delta.`{silver_path}/customer_silver` WHERE data_quality_flag = 'Excellent' LIMIT 5\")\n",
    "    customer_sample.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not show customer sample: {e}\")\n",
    "\n",
    "try:\n",
    "    # Product silver sample\n",
    "    print(\" Product Silver Sample:\")\n",
    "    product_sample = spark.sql(f\"SELECT * FROM delta.`{silver_path}/product_silver` WHERE is_active = true LIMIT 5\")\n",
    "    product_sample.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not show product sample: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Creating Silver Layer Views...\")\n",
    "\n",
    "try:\n",
    "    # Active customers view\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {database_name}.active_customers_vw AS\n",
    "    SELECT *\n",
    "    FROM delta.`{silver_path}/customer_silver`\n",
    "    WHERE is_active = true AND data_quality_flag IN ('Excellent', 'Good')\n",
    "    \"\"\")\n",
    "    print(\"Active customers view created!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create active customers view: {e}\")\n",
    "\n",
    "try:\n",
    "    # Active products view\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {database_name}.active_products_vw AS\n",
    "    SELECT *\n",
    "    FROM delta.`{silver_path}/product_silver`\n",
    "    WHERE is_active = true\n",
    "    \"\"\")\n",
    "    print(\"Active products view created!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create active products view: {e}\")\n",
    "\n",
    "try:\n",
    "    # Clean sales view\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {database_name}.clean_sales_vw AS\n",
    "    SELECT *\n",
    "    FROM delta.`{silver_path}/sales_silver`\n",
    "    WHERE data_quality_flag = 'Good'\n",
    "    \"\"\")\n",
    "    print(\"Clean sales view created!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create clean sales view: {e}\")\n",
    "\n",
    "print(\"Silver layer views creation process completed!\")\n",
    "\n",
    "print(\"Silver Layer Processing Completed!\")\n",
    "\n",
    "# Final record counts\n",
    "try:\n",
    "    sales_count = spark.sql(f\"SELECT COUNT(*) FROM delta.`{silver_path}/sales_silver`\").collect()[0][0]\n",
    "    customer_count = spark.sql(f\"SELECT COUNT(*) FROM delta.`{silver_path}/customer_silver`\").collect()[0][0]\n",
    "    product_count = spark.sql(f\"SELECT COUNT(*) FROM delta.`{silver_path}/product_silver`\").collect()[0][0]\n",
    "\n",
    "    print(f\"\\n Final Record Counts:\")\n",
    "    print(f\"   Sales: {sales_count:,}\")\n",
    "    print(f\"   Customers: {customer_count:,}\")\n",
    "    print(f\"   Products: {product_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get final record counts: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21961c21-574d-442c-9a37-509fe16748e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Silver_Layer_Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}