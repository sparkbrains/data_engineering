{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60241c5-e6a6-4573-9acc-ad24ee747639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver Path: /Volumes/main/retail_lakehouse/silver\nGold Path: /Volumes/main/retail_lakehouse/gold\nUsing database: retail_lakehouse\n\uD83D\uDE80 Starting Gold Layer Aggregation Processing...\nCreating Daily Sales Summary...\nCreating Monthly Sales Summary...\nCreating Product Performance Analysis...\nCreating Customer Analytics...\nCreating Store Performance Analysis...\n✅ Gold layer aggregations completed!\nWriting Daily Sales Summary...\n✅ Daily Sales Summary saved successfully!\nWriting Monthly Sales Summary...\n✅ Monthly Sales Summary saved successfully!\nWriting Product Performance...\n✅ Product Performance saved successfully!\nWriting Customer Analytics...\n✅ Customer Analytics saved successfully!\nWriting Store Performance...\n✅ Store Performance saved successfully!\n✅ Gold layer data saving process completed!\nCreating Gold Layer Delta Tables...\nError creating daily sales summary table: [RequestId=0af24052-e0fd-4f42-abf3-3af765a426e5 ErrorClass=INVALID_PARAMETER_VALUE.INVALID_PARAMETER_VALUE] Missing cloud file system scheme\n\nJVM stacktrace:\ncom.databricks.sql.managedcatalog.UnityCatalogServiceException\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:126)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:266)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7313)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7299)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:6280)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:3242)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$generateTemporaryPathCredentials$2(ProfiledManagedCatalog.scala:744)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1698)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.generateTemporaryPathCredentials(ProfiledManagedCatalog.scala:744)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:146)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:194)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:1190)\n\tat com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:516)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateDSv2Table(ResolveWithCredential.scala:134)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:166)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:69)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:936)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3382)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3211)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:639)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:682)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:624)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\nError creating monthly sales summary table: [RequestId=c6b4fa57-7cad-4e19-8ce6-fb9d46f06d78 ErrorClass=INVALID_PARAMETER_VALUE.INVALID_PARAMETER_VALUE] Missing cloud file system scheme\n\nJVM stacktrace:\ncom.databricks.sql.managedcatalog.UnityCatalogServiceException\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:126)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:266)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7313)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7299)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:6280)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:3242)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$generateTemporaryPathCredentials$2(ProfiledManagedCatalog.scala:744)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1698)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.generateTemporaryPathCredentials(ProfiledManagedCatalog.scala:744)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:146)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:194)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:1190)\n\tat com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:516)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateDSv2Table(ResolveWithCredential.scala:134)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:166)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:69)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$execu\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:936)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3382)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3211)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:639)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:682)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:624)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\nError creating store performance table: [RequestId=bb5611c1-6793-9028-89ce-f8a67361c63b ErrorClass=INVALID_PARAMETER_VALUE.INVALID_PARAMETER_VALUE] Missing cloud file system scheme\n\nJVM stacktrace:\ncom.databricks.sql.managedcatalog.UnityCatalogServiceException\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:126)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:266)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7313)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7299)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:6280)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:3242)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$generateTemporaryPathCredentials$2(ProfiledManagedCatalog.scala:744)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1698)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.generateTemporaryPathCredentials(ProfiledManagedCatalog.scala:744)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:146)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:194)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:1190)\n\tat com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:516)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateDSv2Table(ResolveWithCredential.scala:134)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:166)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:69)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:936)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3382)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3211)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:639)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:682)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:624)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\n✅ Gold layer tables creation process completed!\n=== GOLD LAYER DATA VALIDATION ===\n\n\uD83D\uDCCA Daily Sales Summary Validation:\n   Records: 731\n+----------------+-------------+------------------+\n|transaction_date|total_revenue|total_transactions|\n+----------------+-------------+------------------+\n|      2025-08-12|      4901.58|                 6|\n|      2025-08-11|      3460.06|                 8|\n|      2025-08-10|      8280.52|                 9|\n+----------------+-------------+------------------+\n\n\uD83D\uDCC8 Monthly Sales Summary Validation:\n   Records: 25\n+----------+-------------+----------------------+\n|year_month|total_revenue|revenue_growth_percent|\n+----------+-------------+----------------------+\n|   2025-08|     51250.12|                -59.06|\n|   2025-07|    125185.86|                  5.67|\n|   2025-06|    118472.77|                  3.63|\n+----------+-------------+----------------------+\n\n\uD83D\uDECD️ Top Performing Products:\n+------------+-------------+-------------+-------------------+\n|product_name|     category|total_revenue|total_quantity_sold|\n+------------+-------------+-------------+-------------------+\n|    Curtains|Home & Garden|      3393.84|                 11|\n|   Bed Sheet|Home & Garden|      2673.76|                  9|\n|    Dumbbell|       Sports|      2333.99|                  6|\n|   Bed Sheet|Home & Garden|      2278.95|                  9|\n|    Yoga Mat|       Sports|      2164.62|                  6|\n+------------+-------------+-------------+-------------------+\n\n\uD83D\uDC65 Top Customers by Lifetime Value:\n+----------+---------+--------------------+------------------+\n|first_name|last_name|total_lifetime_value|total_transactions|\n+----------+---------+--------------------+------------------+\n|     Lacey| Lawrence|             2993.12|                 2|\n|     Craig|   Hansen|              2391.6|                 1|\n|     Kevin|  Wallace|              2357.5|                 1|\n|    Warren|   Tucker|              2354.0|                 1|\n|     Danny|    Lewis|              2352.9|                 1|\n+----------+---------+--------------------+------------------+\n\n=== BUSINESS INSIGHTS SUMMARY ===\n\n\uD83C\uDFC6 Overall Performance Metrics:\n+----------+-------------+-----------------+------------------+---------------------+\n|total_days|total_revenue|avg_daily_revenue|total_transactions|avg_transaction_value|\n+----------+-------------+-----------------+------------------+---------------------+\n|       731|    2828720.3|          3869.66|              3926|               716.78|\n+----------+-------------+-----------------+------------------+---------------------+\n\n\uD83D\uDCCA Revenue by Category:\n+-------------+----------------+------------------+\n|     category|category_revenue|revenue_percentage|\n+-------------+----------------+------------------+\n|Home & Garden|        12745.95|             17.25|\n|   Automotive|         9788.86|             13.25|\n|  Electronics|         9738.77|             13.18|\n|       Sports|         9234.54|              12.5|\n|       Beauty|         9083.02|             12.29|\n|     Clothing|         8922.71|             12.08|\n|         Toys|         8852.38|             11.98|\n|        Books|         5525.82|              7.48|\n+-------------+----------------+------------------+\n\n\uD83D\uDC65 Customer Segment Analysis:\n+----------------+--------------+------------------+------------------+--------------------------+\n|customer_segment|customer_count|avg_lifetime_value|total_transactions|total_revenue_contribution|\n+----------------+--------------+------------------+------------------+--------------------------+\n|         Premium|          1215|             89.68|               147|                 108964.98|\n|        Standard|          1303|             77.37|               137|                 100808.14|\n|             Vip|          1220|             77.13|               139|                  94092.54|\n|          Budget|          1262|             62.16|               123|                  78450.36|\n+----------------+--------------+------------------+------------------+--------------------------+\n\n\uD83C\uDF89 Gold Layer Processing Completed!\n\n\uD83D\uDCCB Summary:\n  ✅ Daily sales summary created\n  ✅ Monthly sales summary created\n  ✅ Product performance analysis completed\n  ✅ Customer analytics generated\n  ✅ Store performance metrics calculated\n  ✅ Business insights extracted\n  ✅ Ready for reporting and dashboarding!\n\n\uD83D\uDCCA Final Gold Layer Record Counts:\n   Daily Sales Summary: 731\n   Monthly Sales Summary: 25\n   Product Performance: 178\n   Customer Analytics: 5,000\n   Store Performance: 20\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "# Configuration\n",
    "catalog_name = \"main\"\n",
    "database_name = \"retail_lakehouse\"\n",
    "\n",
    "# Paths\n",
    "silver_path = f\"/Volumes/{catalog_name}/{database_name}/silver\"\n",
    "gold_path = f\"/Volumes/{catalog_name}/{database_name}/gold\"\n",
    "\n",
    "# Create gold directory\n",
    "try:\n",
    "    dbutils.fs.mkdirs(gold_path)\n",
    "except:\n",
    "    print(\"Note: dbutils not available - running in local environment\")\n",
    "\n",
    "# Use the database\n",
    "try:\n",
    "    spark.sql(f\"USE {database_name}\")\n",
    "except:\n",
    "    print(f\"Note: Database {database_name} may not exist yet\")\n",
    "\n",
    "print(f\"Silver Path: {silver_path}\")\n",
    "print(f\"Gold Path: {gold_path}\")\n",
    "print(f\"Using database: {database_name}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def create_daily_sales_summary():\n",
    "    \"\"\"Create daily sales summary for executive reporting\"\"\"\n",
    "    \n",
    "    print(\"Creating Daily Sales Summary...\")\n",
    "    \n",
    "    daily_sales = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        transaction_date,\n",
    "        year,\n",
    "        month,\n",
    "        month_name,\n",
    "        quarter,\n",
    "        day_name,\n",
    "        is_weekend,\n",
    "        \n",
    "        -- Transaction Metrics\n",
    "        COUNT(*) as total_transactions,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers,\n",
    "        COUNT(DISTINCT product_id) as unique_products,\n",
    "        COUNT(DISTINCT store_id) as active_stores,\n",
    "        \n",
    "        -- Revenue Metrics\n",
    "        ROUND(SUM(final_amount), 2) as total_revenue,\n",
    "        ROUND(AVG(final_amount), 2) as avg_transaction_value,\n",
    "        ROUND(MIN(final_amount), 2) as min_transaction_value,\n",
    "        ROUND(MAX(final_amount), 2) as max_transaction_value,\n",
    "        \n",
    "        -- Quantity Metrics\n",
    "        SUM(quantity) as total_items_sold,\n",
    "        ROUND(AVG(quantity), 2) as avg_items_per_transaction,\n",
    "        \n",
    "        -- Discount Metrics\n",
    "        ROUND(SUM(discount_amount), 2) as total_discounts,\n",
    "        ROUND(AVG(discount_percent), 2) as avg_discount_percent,\n",
    "        COUNT(CASE WHEN discount_percent > 0 THEN 1 END) as discounted_transactions,\n",
    "        \n",
    "        -- Estimated Profit\n",
    "        ROUND(SUM(estimated_profit), 2) as estimated_total_profit,\n",
    "        ROUND(AVG(estimated_profit), 2) as avg_profit_per_transaction,\n",
    "        \n",
    "        -- Customer Segments\n",
    "        COUNT(CASE WHEN customer_segment = 'Premium' THEN 1 END) as premium_transactions,\n",
    "        COUNT(CASE WHEN customer_segment = 'Standard' THEN 1 END) as standard_transactions,\n",
    "        COUNT(CASE WHEN customer_segment = 'Budget' THEN 1 END) as budget_transactions,\n",
    "        COUNT(CASE WHEN customer_segment = 'VIP' THEN 1 END) as vip_transactions,\n",
    "        \n",
    "        -- Transaction Size Distribution\n",
    "        COUNT(CASE WHEN transaction_size = 'Small' THEN 1 END) as small_transactions,\n",
    "        COUNT(CASE WHEN transaction_size = 'Medium' THEN 1 END) as medium_transactions,\n",
    "        COUNT(CASE WHEN transaction_size = 'Large' THEN 1 END) as large_transactions,\n",
    "        COUNT(CASE WHEN transaction_size = 'Very Large' THEN 1 END) as very_large_transactions,\n",
    "        \n",
    "        -- Time of Day Distribution\n",
    "        COUNT(CASE WHEN time_of_day = 'Morning' THEN 1 END) as morning_transactions,\n",
    "        COUNT(CASE WHEN time_of_day = 'Afternoon' THEN 1 END) as afternoon_transactions,\n",
    "        COUNT(CASE WHEN time_of_day = 'Evening' THEN 1 END) as evening_transactions,\n",
    "        COUNT(CASE WHEN time_of_day = 'Night' THEN 1 END) as night_transactions,\n",
    "        \n",
    "        -- Processing timestamp\n",
    "        CURRENT_TIMESTAMP() as gold_processing_time\n",
    "        \n",
    "    FROM delta.`{silver_path}/sales_silver`\n",
    "    WHERE data_quality_flag = 'Good'\n",
    "    GROUP BY transaction_date, year, month, month_name, quarter, day_name, is_weekend\n",
    "    ORDER BY transaction_date DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    return daily_sales\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def create_monthly_sales_summary():\n",
    "    \"\"\"Create monthly sales summary for strategic planning\"\"\"\n",
    "    \n",
    "    print(\"Creating Monthly Sales Summary...\")\n",
    "    \n",
    "    monthly_sales = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        year,\n",
    "        month,\n",
    "        month_name,\n",
    "        quarter,\n",
    "        CONCAT(year, '-', LPAD(CAST(month AS STRING), 2, '0')) as year_month,\n",
    "        \n",
    "        -- Transaction Metrics\n",
    "        COUNT(*) as total_transactions,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers,\n",
    "        COUNT(DISTINCT product_id) as unique_products,\n",
    "        COUNT(DISTINCT transaction_date) as active_days,\n",
    "        ROUND(COUNT(*) * 1.0 / COUNT(DISTINCT transaction_date), 2) as avg_transactions_per_day,\n",
    "        \n",
    "        -- Revenue Metrics\n",
    "        ROUND(SUM(final_amount), 2) as total_revenue,\n",
    "        ROUND(AVG(final_amount), 2) as avg_transaction_value,\n",
    "        ROUND(SUM(final_amount) / COUNT(DISTINCT transaction_date), 2) as avg_daily_revenue,\n",
    "        \n",
    "        -- Growth Metrics (will be calculated via window functions)\n",
    "        LAG(SUM(final_amount)) OVER (ORDER BY year, month) as prev_month_revenue,\n",
    "        ROUND((SUM(final_amount) - LAG(SUM(final_amount)) OVER (ORDER BY year, month)) / \n",
    "              NULLIF(LAG(SUM(final_amount)) OVER (ORDER BY year, month), 0) * 100, 2) as revenue_growth_percent,\n",
    "        \n",
    "        -- Customer Metrics\n",
    "        ROUND(SUM(final_amount) / NULLIF(COUNT(DISTINCT customer_id), 0), 2) as revenue_per_customer,\n",
    "        COUNT(*) * 1.0 / NULLIF(COUNT(DISTINCT customer_id), 0) as avg_transactions_per_customer,\n",
    "        \n",
    "        -- Product Performance\n",
    "        SUM(quantity) as total_items_sold,\n",
    "        ROUND(SUM(final_amount) / NULLIF(SUM(quantity), 0), 2) as avg_price_per_item,\n",
    "        \n",
    "        -- Profitability\n",
    "        ROUND(SUM(estimated_profit), 2) as estimated_total_profit,\n",
    "        ROUND(SUM(estimated_profit) / NULLIF(SUM(final_amount), 0) * 100, 2) as profit_margin_percent,\n",
    "        \n",
    "        -- Discount Impact\n",
    "        ROUND(SUM(discount_amount), 2) as total_discounts_given,\n",
    "        ROUND(SUM(discount_amount) / NULLIF(SUM(final_amount), 0) * 100, 2) as discount_percentage_of_revenue,\n",
    "        \n",
    "        CURRENT_TIMESTAMP() as gold_processing_time\n",
    "        \n",
    "    FROM delta.`{silver_path}/sales_silver`\n",
    "    WHERE data_quality_flag = 'Good'\n",
    "    GROUP BY year, month, month_name, quarter\n",
    "    ORDER BY year DESC, month DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    return monthly_sales\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def create_product_performance():\n",
    "    \"\"\"Create product performance summary\"\"\"\n",
    "    \n",
    "    print(\"Creating Product Performance Analysis...\")\n",
    "    \n",
    "    product_performance = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        p.product_id,\n",
    "        p.product_name,\n",
    "        p.category,\n",
    "        p.sub_category,\n",
    "        p.brand,\n",
    "        p.price_category,\n",
    "        p.selling_price,\n",
    "        p.cost_price,\n",
    "        p.margin_percentage,\n",
    "        \n",
    "        -- Sales Performance\n",
    "        COUNT(s.transaction_id) as total_transactions,\n",
    "        COALESCE(SUM(s.quantity), 0) as total_quantity_sold,\n",
    "        COALESCE(ROUND(SUM(s.final_amount), 2), 0.0) as total_revenue,\n",
    "        COALESCE(ROUND(AVG(s.final_amount), 2), 0.0) as avg_transaction_value,\n",
    "        \n",
    "        -- Time-based metrics\n",
    "        MIN(s.transaction_date) as first_sale_date,\n",
    "        MAX(s.transaction_date) as last_sale_date,\n",
    "        COALESCE(DATEDIFF(MAX(s.transaction_date), MIN(s.transaction_date)), 0) as days_in_market,\n",
    "        \n",
    "        -- Customer reach\n",
    "        COUNT(DISTINCT s.customer_id) as unique_customers,\n",
    "        COUNT(DISTINCT s.store_id) as stores_sold_in,\n",
    "        \n",
    "        -- Performance ratios\n",
    "        COALESCE(ROUND(SUM(s.final_amount) / NULLIF(COUNT(s.transaction_id), 0), 2), 0.0) as revenue_per_transaction,\n",
    "        COALESCE(ROUND(SUM(s.quantity) / NULLIF(COUNT(s.transaction_id), 0), 2), 0.0) as avg_quantity_per_transaction,\n",
    "        COALESCE(ROUND(SUM(s.final_amount) / NULLIF(SUM(s.quantity), 0), 2), 0.0) as avg_selling_price,\n",
    "        \n",
    "        -- Profitability\n",
    "        COALESCE(ROUND(SUM(s.estimated_profit), 2), 0.0) as total_estimated_profit,\n",
    "        COALESCE(ROUND(SUM(s.estimated_profit) / NULLIF(SUM(s.final_amount), 0) * 100, 2), 0.0) as profit_margin_realized,\n",
    "        \n",
    "        -- Ranking metrics (will be added via window functions)\n",
    "        ROW_NUMBER() OVER (ORDER BY SUM(s.final_amount) DESC NULLS LAST) as revenue_rank,\n",
    "        ROW_NUMBER() OVER (ORDER BY SUM(s.quantity) DESC NULLS LAST) as quantity_rank,\n",
    "        ROW_NUMBER() OVER (ORDER BY COUNT(s.transaction_id) DESC NULLS LAST) as transaction_rank,\n",
    "        \n",
    "        CURRENT_TIMESTAMP() as gold_processing_time\n",
    "        \n",
    "    FROM delta.`{silver_path}/product_silver` p\n",
    "    LEFT JOIN delta.`{silver_path}/sales_silver` s ON p.product_id = s.product_id\n",
    "    WHERE s.data_quality_flag = 'Good' OR s.data_quality_flag IS NULL\n",
    "    GROUP BY p.product_id, p.product_name, p.category, p.sub_category, p.brand, \n",
    "             p.price_category, p.selling_price, p.cost_price, p.margin_percentage\n",
    "    ORDER BY total_revenue DESC NULLS LAST\n",
    "    \"\"\")\n",
    "    \n",
    "    return product_performance\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Customer Analytics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def create_customer_analytics():\n",
    "    \"\"\"Create customer behavior analytics\"\"\"\n",
    "    \n",
    "    print(\"Creating Customer Analytics...\")\n",
    "    \n",
    "    customer_analytics = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        c.email,\n",
    "        c.age,\n",
    "        c.age_group,\n",
    "        c.customer_segment,\n",
    "        c.tenure_category,\n",
    "        c.customer_tenure_years,\n",
    "        c.registration_date,\n",
    "        \n",
    "        -- Purchase Behavior\n",
    "        COUNT(s.transaction_id) as total_transactions,\n",
    "        COALESCE(SUM(s.quantity), 0) as total_items_purchased,\n",
    "        COALESCE(ROUND(SUM(s.final_amount), 2), 0.0) as total_lifetime_value,\n",
    "        COALESCE(ROUND(AVG(s.final_amount), 2), 0.0) as avg_transaction_value,\n",
    "        COALESCE(ROUND(SUM(s.final_amount) / NULLIF(COUNT(s.transaction_id), 0), 2), 0.0) as avg_order_value,\n",
    "        \n",
    "        -- Frequency Analysis\n",
    "        MIN(s.transaction_date) as first_purchase_date,\n",
    "        MAX(s.transaction_date) as last_purchase_date,\n",
    "        COALESCE(DATEDIFF(MAX(s.transaction_date), MIN(s.transaction_date)), 0) as purchase_span_days,\n",
    "        COALESCE(ROUND(COUNT(s.transaction_id) * 30.0 / NULLIF(DATEDIFF(MAX(s.transaction_date), MIN(s.transaction_date)) + 1, 0), 2), 0.0) as avg_purchases_per_month,\n",
    "        \n",
    "        -- Recency, Frequency, Monetary (RFM) Components\n",
    "        COALESCE(DATEDIFF(CURRENT_DATE(), MAX(s.transaction_date)), 9999) as days_since_last_purchase,\n",
    "        COUNT(s.transaction_id) as purchase_frequency,\n",
    "        COALESCE(ROUND(SUM(s.final_amount), 2), 0.0) as monetary_value,\n",
    "        \n",
    "        -- Product Preferences\n",
    "        COUNT(DISTINCT s.product_id) as unique_products_purchased,\n",
    "        COUNT(DISTINCT s.category) as unique_categories_purchased,\n",
    "        \n",
    "        -- Store Preferences\n",
    "        COUNT(DISTINCT s.store_id) as unique_stores_visited,\n",
    "        \n",
    "        -- Discount Usage\n",
    "        COUNT(CASE WHEN s.discount_percent > 0 THEN 1 END) as discounted_purchases,\n",
    "        COALESCE(ROUND(COUNT(CASE WHEN s.discount_percent > 0 THEN 1 END) * 100.0 / NULLIF(COUNT(s.transaction_id), 0), 2), 0.0) as discount_usage_rate,\n",
    "        COALESCE(ROUND(AVG(s.discount_percent), 2), 0.0) as avg_discount_percent,\n",
    "        \n",
    "        -- Seasonality (quarters with purchases)\n",
    "        COUNT(DISTINCT s.quarter) as quarters_active,\n",
    "        \n",
    "        -- Customer Value Segment\n",
    "        CASE \n",
    "            WHEN SUM(s.final_amount) >= 1000 AND COUNT(s.transaction_id) >= 10 THEN 'High Value High Frequency'\n",
    "            WHEN SUM(s.final_amount) >= 1000 AND COUNT(s.transaction_id) < 10 THEN 'High Value Low Frequency'\n",
    "            WHEN SUM(s.final_amount) < 1000 AND COUNT(s.transaction_id) >= 10 THEN 'Low Value High Frequency'\n",
    "            WHEN SUM(s.final_amount) < 1000 AND COUNT(s.transaction_id) < 10 THEN 'Low Value Low Frequency'\n",
    "            ELSE 'Other'\n",
    "        END as customer_value_segment,\n",
    "        \n",
    "        CURRENT_TIMESTAMP() as gold_processing_time\n",
    "        \n",
    "    FROM delta.`{silver_path}/customer_silver` c\n",
    "    LEFT JOIN delta.`{silver_path}/sales_silver` s ON c.customer_id = s.customer_id\n",
    "    WHERE c.data_quality_flag IN ('Excellent', 'Good') OR c.data_quality_flag IS NULL\n",
    "    GROUP BY c.customer_id, c.first_name, c.last_name, c.email, c.age, c.age_group,\n",
    "             c.customer_segment, c.tenure_category, c.customer_tenure_years, c.registration_date\n",
    "    ORDER BY total_lifetime_value DESC NULLS LAST\n",
    "    \"\"\")\n",
    "    \n",
    "    return customer_analytics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def create_store_performance():\n",
    "    \"\"\"Create store performance summary\"\"\"\n",
    "    \n",
    "    print(\"Creating Store Performance Analysis...\")\n",
    "    \n",
    "    store_performance = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        store_id,\n",
    "        store_name,\n",
    "        region,\n",
    "        \n",
    "        -- Transaction Metrics\n",
    "        COUNT(*) as total_transactions,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers,\n",
    "        COUNT(DISTINCT product_id) as unique_products,\n",
    "        \n",
    "        -- Revenue Metrics\n",
    "        ROUND(SUM(final_amount), 2) as total_revenue,\n",
    "        ROUND(AVG(final_amount), 2) as avg_transaction_value,\n",
    "        ROUND(MIN(final_amount), 2) as min_transaction_value,\n",
    "        ROUND(MAX(final_amount), 2) as max_transaction_value,\n",
    "        \n",
    "        -- Quantity Metrics\n",
    "        SUM(quantity) as total_items_sold,\n",
    "        ROUND(AVG(quantity), 2) as avg_items_per_transaction,\n",
    "        \n",
    "        -- Time-based metrics\n",
    "        MIN(transaction_date) as first_transaction_date,\n",
    "        MAX(transaction_date) as last_transaction_date,\n",
    "        COUNT(DISTINCT transaction_date) as active_days,\n",
    "        \n",
    "        -- Customer Segments\n",
    "        COUNT(CASE WHEN customer_segment = 'Premium' THEN 1 END) as premium_transactions,\n",
    "        COUNT(CASE WHEN customer_segment = 'Standard' THEN 1 END) as standard_transactions,\n",
    "        COUNT(CASE WHEN customer_segment = 'Budget' THEN 1 END) as budget_transactions,\n",
    "        COUNT(CASE WHEN customer_segment = 'VIP' THEN 1 END) as vip_transactions,\n",
    "        \n",
    "        -- Performance ratios\n",
    "        ROUND(COUNT(*) * 1.0 / COUNT(DISTINCT transaction_date), 2) as avg_transactions_per_day,\n",
    "        ROUND(SUM(final_amount) / COUNT(DISTINCT transaction_date), 2) as avg_daily_revenue,\n",
    "        ROUND(SUM(final_amount) / COUNT(DISTINCT customer_id), 2) as revenue_per_customer,\n",
    "        \n",
    "        -- Ranking metrics\n",
    "        ROW_NUMBER() OVER (ORDER BY SUM(final_amount) DESC) as revenue_rank_by_region,\n",
    "        ROW_NUMBER() OVER (PARTITION BY region ORDER BY SUM(final_amount) DESC) as revenue_rank_within_region,\n",
    "        \n",
    "        CURRENT_TIMESTAMP() as gold_processing_time\n",
    "        \n",
    "    FROM delta.`{silver_path}/sales_silver`\n",
    "    WHERE data_quality_flag = 'Good'\n",
    "    GROUP BY store_id, store_name, region\n",
    "    ORDER BY total_revenue DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    return store_performance\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Process all aggregations\n",
    "print(\"Starting Gold Layer Aggregation Processing...\")\n",
    "\n",
    "# Create daily sales summary\n",
    "daily_sales_df = create_daily_sales_summary()\n",
    "\n",
    "# Create monthly sales summary\n",
    "monthly_sales_df = create_monthly_sales_summary()\n",
    "\n",
    "# Create product performance analysis\n",
    "product_performance_df = create_product_performance()\n",
    "\n",
    "# Create customer analytics\n",
    "customer_analytics_df = create_customer_analytics()\n",
    "\n",
    "# Create store performance analysis\n",
    "store_performance_df = create_store_performance()\n",
    "\n",
    "print Gold layer aggregations completed!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Write Gold Layer Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Write daily sales summary\n",
    "print(\"Writing Daily Sales Summary...\")\n",
    "try:\n",
    "    if daily_sales_df.count() > 0:\n",
    "        (daily_sales_df\n",
    "         .write\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .format(\"delta\")\n",
    "         .save(f\"{gold_path}/daily_sales_summary\")\n",
    "        )\n",
    "        print(\" Daily Sales Summary saved successfully!\")\n",
    "    else:\n",
    "        print(\"No daily sales data to write\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing daily sales summary: {e}\")\n",
    "\n",
    "# Write monthly sales summary\n",
    "print(\"Writing Monthly Sales Summary...\")\n",
    "try:\n",
    "    if monthly_sales_df.count() > 0:\n",
    "        (monthly_sales_df\n",
    "         .write\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .format(\"delta\")\n",
    "         .save(f\"{gold_path}/monthly_sales_summary\")\n",
    "        )\n",
    "        print(\"Monthly Sales Summary saved successfully!\")\n",
    "    else:\n",
    "        print(\" No monthly sales data to write\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing monthly sales summary: {e}\")\n",
    "\n",
    "# Write product performance\n",
    "print(\"Writing Product Performance...\")\n",
    "try:\n",
    "    if product_performance_df.count() > 0:\n",
    "        (product_performance_df\n",
    "         .write\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .format(\"delta\")\n",
    "         .save(f\"{gold_path}/product_performance\")\n",
    "        )\n",
    "        print(\" Product Performance saved successfully!\")\n",
    "    else:\n",
    "        print(\" No product performance data to write\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing product performance: {e}\")\n",
    "\n",
    "# Write customer analytics\n",
    "print(\"Writing Customer Analytics...\")\n",
    "try:\n",
    "    if customer_analytics_df.count() > 0:\n",
    "        (customer_analytics_df\n",
    "         .write\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .format(\"delta\")\n",
    "         .save(f\"{gold_path}/customer_analytics\")\n",
    "        )\n",
    "        print(\" Customer Analytics saved successfully!\")\n",
    "    else:\n",
    "        print(\" No customer analytics data to write\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing customer analytics: {e}\")\n",
    "\n",
    "# Write store performance\n",
    "print(\"Writing Store Performance...\")\n",
    "try:\n",
    "    if store_performance_df.count() > 0:\n",
    "        (store_performance_df\n",
    "         .write\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .format(\"delta\")\n",
    "         .save(f\"{gold_path}/store_performance\")\n",
    "        )\n",
    "        print(\" Store Performance saved successfully!\")\n",
    "    else:\n",
    "        print(\" No store performance data to write\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing store performance: {e}\")\n",
    "\n",
    "print(\" Gold layer data saving process completed!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create Delta tables for gold layer\n",
    "print(\"Creating Gold Layer Delta Tables...\")\n",
    "\n",
    "try:\n",
    "    # Daily Sales Summary Table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {database_name}.daily_sales_summary\n",
    "    USING DELTA\n",
    "    LOCATION '{gold_path}/daily_sales_summary'\n",
    "    \"\"\")\n",
    "    print(\" Daily Sales Summary table created!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating daily sales summary table: {e}\")\n",
    "\n",
    "try:\n",
    "    # Monthly Sales Summary Table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {database_name}.monthly_sales_summary\n",
    "    USING DELTA\n",
    "    LOCATION '{gold_path}/monthly_sales_summary'\n",
    "    \"\"\")\n",
    "    print(\" Monthly Sales Summary table created!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating monthly sales summary table: {e}\")\n",
    "\n",
    "try:\n",
    "    # Product Performance Table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {database_name}.product_performance\n",
    "    USING DELTA\n",
    "    LOCATION '{gold_path}/product_performance'\n",
    "    \"\"\")\n",
    "    print(\" Product Performance table created!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating product performance table: {e}\")\n",
    "\n",
    "try:\n",
    "    # Customer Analytics Table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {database_name}.customer_analytics\n",
    "    USING DELTA\n",
    "    LOCATION '{gold_path}/customer_analytics'\n",
    "    \"\"\")\n",
    "    print(\" Customer Analytics table created!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating customer analytics table: {e}\")\n",
    "\n",
    "try:\n",
    "    # Store Performance Table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {database_name}.store_performance\n",
    "    USING DELTA\n",
    "    LOCATION '{gold_path}/store_performance'\n",
    "    \"\"\")\n",
    "    print(\" Store Performance table created!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating store performance table: {e}\")\n",
    "\n",
    "print(\" Gold layer tables creation process completed!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Validate gold layer data\n",
    "print(\"=== GOLD LAYER DATA VALIDATION ===\\n\")\n",
    "\n",
    "try:\n",
    "    # Daily Sales Summary validation\n",
    "    print(\"\uD83D\uDCCA Daily Sales Summary Validation:\")\n",
    "    daily_count = spark.sql(f\"SELECT COUNT(*) as count FROM delta.`{gold_path}/daily_sales_summary`\").collect()[0]['count']\n",
    "    print(f\"   Records: {daily_count:,}\")\n",
    "    \n",
    "    if daily_count > 0:\n",
    "        daily_sample = spark.sql(f\"SELECT transaction_date, total_revenue, total_transactions FROM delta.`{gold_path}/daily_sales_summary` ORDER BY transaction_date DESC LIMIT 3\")\n",
    "        daily_sample.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error validating daily sales summary: {e}\")\n",
    "\n",
    "try:\n",
    "    # Monthly Sales Summary validation\n",
    "    print(\"\uD83D\uDCC8 Monthly Sales Summary Validation:\")\n",
    "    monthly_count = spark.sql(f\"SELECT COUNT(*) as count FROM delta.`{gold_path}/monthly_sales_summary`\").collect()[0]['count']\n",
    "    print(f\"   Records: {monthly_count:,}\")\n",
    "    \n",
    "    if monthly_count > 0:\n",
    "        monthly_sample = spark.sql(f\"SELECT year_month, total_revenue, revenue_growth_percent FROM delta.`{gold_path}/monthly_sales_summary` ORDER BY year DESC, month DESC LIMIT 3\")\n",
    "        monthly_sample.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error validating monthly sales summary: {e}\")\n",
    "\n",
    "try:\n",
    "    # Top performing products\n",
    "    print(\"\uD83D\uDECD️ Top Performing Products:\")\n",
    "    top_products = spark.sql(f\"\"\"\n",
    "    SELECT product_name, category, total_revenue, total_quantity_sold \n",
    "    FROM delta.`{gold_path}/product_performance` \n",
    "    ORDER BY total_revenue DESC \n",
    "    LIMIT 5\n",
    "    \"\"\")\n",
    "    top_products.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error showing top products: {e}\")\n",
    "\n",
    "try:\n",
    "    # Top customers by lifetime value\n",
    "    print(\"\uD83D\uDC65 Top Customers by Lifetime Value:\")\n",
    "    top_customers = spark.sql(f\"\"\"\n",
    "    SELECT first_name, last_name, total_lifetime_value, total_transactions\n",
    "    FROM delta.`{gold_path}/customer_analytics`\n",
    "    ORDER BY total_lifetime_value DESC\n",
    "    LIMIT 5\n",
    "    \"\"\")\n",
    "    top_customers.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error showing top customers: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Business Insights Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== BUSINESS INSIGHTS SUMMARY ===\\n\")\n",
    "\n",
    "try:\n",
    "    # Overall performance metrics\n",
    "    print(\" Overall Performance Metrics:\")\n",
    "    overall_metrics = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_days,\n",
    "        ROUND(SUM(total_revenue), 2) as total_revenue,\n",
    "        ROUND(AVG(total_revenue), 2) as avg_daily_revenue,\n",
    "        SUM(total_transactions) as total_transactions,\n",
    "        ROUND(AVG(avg_transaction_value), 2) as avg_transaction_value\n",
    "    FROM delta.`{gold_path}/daily_sales_summary`\n",
    "    \"\"\")\n",
    "    overall_metrics.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating overall metrics: {e}\")\n",
    "\n",
    "try:\n",
    "    # Revenue by category\n",
    "    print(\"Revenue by Category:\")\n",
    "    category_revenue = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        ROUND(SUM(total_revenue), 2) as category_revenue,\n",
    "        ROUND(SUM(total_revenue) * 100.0 / (SELECT SUM(total_revenue) FROM delta.`{gold_path}/product_performance`), 2) as revenue_percentage\n",
    "    FROM delta.`{gold_path}/product_performance`\n",
    "    GROUP BY category\n",
    "    ORDER BY category_revenue DESC\n",
    "    \"\"\")\n",
    "    category_revenue.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating category revenue: {e}\")\n",
    "\n",
    "try:\n",
    "    # Customer segment analysis\n",
    "    print(\" Customer Segment Analysis:\")\n",
    "    segment_analysis = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_segment,\n",
    "        COUNT(*) as customer_count,\n",
    "        ROUND(AVG(total_lifetime_value), 2) as avg_lifetime_value,\n",
    "        SUM(total_transactions) as total_transactions,\n",
    "        ROUND(SUM(total_lifetime_value), 2) as total_revenue_contribution\n",
    "    FROM delta.`{gold_path}/customer_analytics`\n",
    "    GROUP BY customer_segment\n",
    "    ORDER BY avg_lifetime_value DESC\n",
    "    \"\"\")\n",
    "    segment_analysis.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing customer segments: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\" Gold Layer Processing Completed!\")\n",
    "print(\"\\n Summary:\")\n",
    "print(\" Daily sales summary created\")\n",
    "print(\" Monthly sales summary created\")\n",
    "print(\" Product performance analysis completed\")\n",
    "print(\" Customer analytics generated\")\n",
    "print(\" Store performance metrics calculated\")\n",
    "print(\" Business insights extracted\")\n",
    "print(\" Ready for reporting and dashboarding!\")\n",
    "\n",
    "# Final record counts\n",
    "try:\n",
    "    daily_count = spark.sql(f\"SELECT COUNT(*) FROM delta.`{gold_path}/daily_sales_summary`\").collect()[0][0]\n",
    "    monthly_count = spark.sql(f\"SELECT COUNT(*) FROM delta.`{gold_path}/monthly_sales_summary`\").collect()[0][0]\n",
    "    product_count = spark.sql(f\"SELECT COUNT(*) FROM delta.`{gold_path}/product_performance`\").collect()[0][0]\n",
    "    customer_count = spark.sql(f\"SELECT COUNT(*) FROM delta.`{gold_path}/customer_analytics`\").collect()[0][0]\n",
    "    store_count = spark.sql(f\"SELECT COUNT(*) FROM delta.`{gold_path}/store_performance`\").collect()[0][0]\n",
    "\n",
    "    print(f\"\\n Final Gold Layer Record Counts:\")\n",
    "    print(f\"   Daily Sales Summary: {daily_count:,}\")\n",
    "    print(f\"   Monthly Sales Summary: {monthly_count:,}\")\n",
    "    print(f\"   Product Performance: {product_count:,}\")\n",
    "    print(f\"   Customer Analytics: {customer_count:,}\")\n",
    "    print(f\"   Store Performance: {store_count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get final record counts: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d47c2ba0-f8ea-4335-a802-50facb174fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Gold_Layer_Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}